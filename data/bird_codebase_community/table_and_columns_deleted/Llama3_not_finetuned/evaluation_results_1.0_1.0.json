{"SELECT * FROM logs;": [[44620, 2, "2012-11-28 17:15:34.0", 1, "eplace('<p>Could you use some sort of generalize additive model, where the dependent variables are relatedto the predictors as a smooth function, like done in the gam() function in R</p>\\n','\\n',char(10)", 8762, "2012-11-28 17:15:34.0", 0, 44616]], "SELECT * FROM comments;": [[10616, 101, "2012-04-15 01:44:53.0", "Jonathan Callen", "2014-03-12 23:35:47.0", "http://example.com/", "United States", "eplace('<p>.</p>\\n','\\n',char(10)", 0, 0, 0, 913608, 27]], "SELECT * FROM Comments;": [[107915, 2, 33815, "ab296b9b-e71d-4140-ace4-9945f4dfccd6", "2012-08-07 02:57:20.0", 3262, "X and Y are not correlated (-.01); however, when I place X in a multiple regression predicting Y, alongside three other (related) variables, X and two other variables are significant predictors of Y. Note that the two other variables are significantly correlated with Y.\\\\n\\\\nHow should I interpret these findings? X predicts unique variance in Y, but since these are not correlated, it is somehow difficult to interpret. \\\\n\\\\nI know of opposite cases (i.e., two variables are correlated but regression is not significant) and those are relatively simpler to understand from a theoretical and statistical perspective.  \\\\n\\\\n\\\\n", "", "", "None", "None", "None", "None", "None", "None", "None", "None"], [177986, "None", 90942, 24808, "2014-03-22 02:11:12.0", "None", "+1. What''s the URL for the 3rd edition of Bayesian Data Analysis?", "None", "None", 0, "None", "None", "None", "None", "None", "None", "None"], [111764, "None", 58210, 25184, "2013-05-05 23:46:57.0", "None", "Your point about the number of penguins and chickens is interesting.  And I get what you''re saying.  However, the way that I read the question, the answer wasn''t intended to be so strongly dependent on the number of chickens or penguins.  What would your answer be if the number of chickens was equal to the number of penguins, and that number was large?", "None", "None", 0, "None", "None", "None", "None", "None", "None", "None"], [290, "None", 28270, 127, "None", "None", "None", "None", "data-imputation", "None", 28269, "None", "None", "None", "None", "None", "None"], [81849, "None", 48444, "None", "2014-06-16 13:45:59.0", "None", "None", "None", "None", "None", "None", "Editor", "None", "None", "None", "None", "None"], [1886, 24, 26, 509577, "2010-11-07 05:14:31.0", 5, "None", "eplace('<p>I am a Analytics consultant at REDWOOD ASSOCIATES. We also provide training on Analytics to students in India.\\nI am also a part time Forex Trader.</p>\\n','\\n',char(10)", "None", 0, "None", 0, "fxnikee", "2010-11-07 05:14:31.0", "http://www.analyticstraining.in", "Bangalore, India", "None"], [44222, 1, "None", 0, "2014-04-21 23:28:01.0", 2, "None", "None", "None", 0, "None", "None", "user144794", "2014-06-08 01:00:17.0", 4361372, "None", "https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1"], [17690, "None", 9912, "None", "2012-03-18 22:36:02.0", "None", "None", "None", "None", "None", "None", "Scholar", "None", "None", "None", "None", "None"], [190065, "None", 97599, 32036, "2014-05-14 01:18:36.0", "None", "How is this different from coding $X_2$ as zero for those who only attend the event?", "None", "None", 0, "None", "None", "None", "None", "None", "None", "None"], [59455, 46889, 2, 6547, "2013-05-19 16:04:03.0", 0, "eplace('<p>A possible very practical approach could be calculate the ratio of the survival function of the distribution $\\\\Pr\\\\left(\\\\tilde X \\\\gt  1- \\\\alpha \\\\right)$ against the normal one, showing it is quite far greater. Another approach can be calculating the ratios of percentiles $w_1=\\\\frac{\\\\tilde{x_{99}}-\\\\tilde{x_{50}}}{\\\\tilde{x_{75}}-\\\\tilde{x_{50}}}$ of the distribution $\\\\tilde x$ under interest and dividing it against the normal one quantile values, $w_2=\\\\frac{\\\\tilde{\\\\Phi_{99}}-\\\\tilde{\\\\Phi_{50}}}{\\\\tilde{\\\\Phi_{75}}-\\\\tilde{\\\\Phi_{50}}}$, $\\\\tau=\\\\frac{w_1}{w_2}$.</p>\\n','\\n',char(10)", "None", "None", 0, "None", "None", "None", "2013-05-19 16:04:03.0", "None", "None", "None"]], "SELECT * FROM Posts;": [[39597, 4, 13607, "7528cb3e-338e-4f2f-aea5-2164e69d1cc7", "2011-07-31 02:37:08.0", 183, "Can non-random samples be analyzed using standard statistical tests?", "edited tags; edited title", ""]], "SELECT * FROM StatisticalModels;": [[1869, "zero-inflated", 1]], "SELECT * FROM Teachers;": [[11416, 5594, "Teacher", "2011-09-23 13:19:21.0"]], "SELECT * FROM lokheart;": [[588, 1034, "2010-07-29 10:36:25.0", "lokheart", "2014-09-05 16:19:18.0", 185, 38, 0, 156553]], "SELECT * FROM CommentTable;": [[3312222, "2014-08-21 17:00:44.0", 112705, 13875, 1, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [55139, "2013-10-04 14:08:17.0", 20587, "None", "None", "Teacher", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [2638298, "2013-09-13 15:28:32.0", "None", "None", 1, 118, 3734, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [906579, "2011-08-05 06:07:07.0", "None", "None", 1, 13873, 12495, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [36777, "2011-12-31 06:46:14.0", "None", "None", 0, 20393, "None", "Good answer. It is worth noting (though obvious) that working with a class of alternatives that is larger than $k$ for some small $k$ can often be computationally prohibitive, let alone if one has to work with an infinite or uncountable number of alternatives, which may also occur in practice. A big plus of the p-value approach is that it is often (usually?) computationally simple/tractable.", 7008, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [3563, "2010-08-02", "None", "None", 2, 1114, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [66518, "2014-02-07 22:31:27.0", "None", "None", "Supporter", 39707, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [27402, "2011-01-21", "None", "None", 2, 6421, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [533130, "2011-05-01 09:28:25.0", "None", "None", 1, 10191, 5617, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [13737, "2011-08-01 23:43:23.0", 13736, "None", 2, 3, 601, "eplace('<p>It sounds like  you''re graphing a histogram.  Convert the y-axis from counts to proportion of the total.  Make sure that the range on the two graphs is the same after this conversion.</p>\\n','\\n',char(10)", "None", "2011-08-02 09:53:40.0", 2, 601, "2011-08-02 09:53:40.0", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [16372, "2011-10-02 23:11:12.0", 16365, "None", 6, 2, 2958, "eplace('<p>All three methods should produce the same result for the house edge.  To check yours, you should find </p>\\n\\n<pre><code>Number 1s     Prob\\n   0        125/216 \\n   1         75/216\\n   2         15/216\\n   3          1/216\\n</code></pre>\\n\\n<p>which would make your expected winnings (for a stake of 1) </p>\\n\\n<p>$$\\\\frac{-1\\\\times 125 + 1\\\\times 75 + 2 \\\\times 15 + 3 \\\\times 1}{216} = - \\\\frac{17}{216} \\\\approx -0.0787.$$</p>\\n\\n<p>You may need to check your calculations again.</p>\\n','\\n',char(10)", 1, "None", "None", "None", "None", "2011-10-02 23:11:12.0", "None", "None", "None", "None", "None", "None", "None", "None"], [1, "2014-08-18 17:38:46.0", 1, 54277, 1, 112348, 54277, 0, "None", "None", "None", "None", "None", "2014-08-30 12:36:07.0", 28, "eplace('<p>I am running a logistic regression model in R using multiply imputed data created using Amelia II, which I am then analyzing using Zelig. I would like to be able to report some measures of goodness-of-fit (e.g. likelihood ratio, pseudo R-squared, Hosmer-Lemeshow), however none are provided in the default Zelig output and I haven''t been able to figure out a way to extract any from the <code>zelig()</code> object. </p>\\n\\n<p>Do measures of goodness-of-fit need to be calculated differently when using multiply imputed datasets? Are there any R packages that are able to do this? I have looked into several packages that provide measures of goodness-of-fit, such as pscl, however they only work on glm objects, not MI objects created when using Amelia and Zelig.</p>\\n\\n<p>Thanks in advance for your help!</p>\\n','\\n',char(10)", "Measures of goodness-of-fit using multiply imputed data in Zelig", "<r><regression><logistic><missing-data><multiple-imputation>", "2014-08-18 18:24:35.0", "None", "None", "None"], [30228, "2011-02-10", "None", "None", 2, 315, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [1435, 54, "None", "None", "None", 46637, 46636, "pymc", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [2495871, "2013-06-04 17:10:54.0", "None", "None", 1, 60856, 44569, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], ["7072cab9-11d4-432e-a4c7-ae9af757f1db", "2011-03-10 04:37:35.0", "None", "None", "None", 21510, 8104, "added 320 characters in body", "None", "None", "None", "None", "None", "None", "None", "##Background\\\\n\\\\nOne of the most commonly used weak prior on variance is the inverse-gamma with parameters $\\\\alpha =0.001, \\\\beta=0.001$ [(Gelman 2006)][1].\\\\n\\\\nHowever, this distribution has a 90%CI of approximately $[3\\\\times10^{19},\\\\infty]$.\\\\n\\\\n    library(pscl)\\\\n    sapply(c(0.05, 0.95), function(x) qigamma(x, 0.001, 0.001))\\\\n\\\\n    [1] 3.362941e+19          Inf\\\\n\\\\nFrom this, I interpret that the $IG(0.001, 0.001)$ gives a low probability that variance will be very high, and the very low probability  that variance will be less than 1 $P(\\\\sigma<1|\\\\alpha=0.001, \\\\beta=0.001)=0.006$.\\\\n\\\\n    pigamma(1, 0.001, 0.001)\\\\n    [1] 0.006312353\\\\n\\\\n\\\\n##Question\\\\n\\\\nAm I missing something or is this actually an informative prior?\\\\n\\\\n*update* to clarify, the reason that I was considering this ''informative'' is because it claims very strongly that the variance is enormous and well beyond the scale of almost any variance ever measured. \\\\n\\\\n*follow-up* would a meta-analysis of a large number of variance estimates provide a more reasonable prior?\\\\n   \\\\n---\\\\n\\\\n##Reference\\\\n\\\\nGelman 2006. [Prior distributions for variance parameters in\\\\nhierarchical models][1]. Bayesian Analysis 1(3):515\u2013533\\\\n\\\\n[1]:http://www.stat.columbia.edu/~gelman/research/published/taumain.pdf", "None", 1381, "None", 5, "", "None"], [22583, "2010-12-09", "None", "None", 2, 4457, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [43721, "2014-04-13 20:04:45.0", "None", "None", 1, 4321906, "None", "None", 0, "None", "None", "None", "None", "None", "user43721", "None", "None", 0, "2014-07-17 18:25:20.0", 5, "None", "https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1"]], "SELECT * FROM Predicted;": [[353908, 2, 105562, "64532f6b-b185-4393-8510-3d25fa07af7c", "2014-07-02 17:28:44.0", 45985, "Cost for mis-classification can be used as below\\\\n\\\\nfit <- rpart(target ~ .,\\\\ndata=dataset, method=\"class\", parms=list( split=\"information\", loss=matrix(c(0,1,2,0), byrow=TRUE, nrow=2)), control=rpart.control(usesurrogate=0,maxsurrogate=0))\\\\n\\\\nIn the above example, I have considered that the cost of mis-classifying a -ve observation as 1 unit and cost of mis-classifying a +ve example as +ve as 2 units.", "", ""]], "SELECT * FROM Users;": [[22946, 12053, "Supporter", "2012-07-13 14:40:15.0", "None", "None", "None", "None", "None", "None", "None"], [48749, "None", 1, "2014-06-20 21:26:13.0", "user3761639", "2014-07-01 00:28:38.0", 0, 0, 0, 4642102, "https://www.gravatar.com/avatar/769a4a9450283f19a6aaa33194038e1f?s=128&d=identicon&r=PG&f=1"]], "SELECT * FROM supporter_comments;": [[27621, 1, "2012-05-03 11:20:49.0", 1, 474, "eplace('<p>I used R <a href=\"http://cran.r-project.org/web/packages/mvpart/index.html\" rel=\"nofollow\">mvpart</a> package to create a multivariate regression tree. This is part of the output:</p>\\n\\n<pre><code>          CP               nsplit         rel error    xerror       xstd\\n1       0.02717093      0         1.0000000 1.0005358 0.03481409\\n2       0.01302184      2         0.9456581 0.9521266 0.03306820\\n\\nNode number 1: 3479 observations,    complexity param=0.02717093\\nMeans=12.94,0.5749,9.375,0.72,1.611,0.973,2.153,0.6209,3.307,3.702,2.422,0.3837,1.499,     Summed MSE=1305.19 \\nleft son=2 (992 obs) right son=3 (2487 obs)\\nPrimary splits:\\n  Dag           splits as  RRLLRRR, improve=0.02478172, (0 missing)\\n  Hoofdberoep    splits as  RLRLLRRLLLLRLL, improve=0.02313676, (0 missing)\\n  Ploegenstelsel splits as  RRLRRRL, improve=0.02191660, (0 missing)\\n  Werksituatie   splits as  LRLR, improve=0.02179270, (0 missing)\\n  Werkuren       splits as  RLRRRR, improve=0.02130351, (0 missing)\\n</code></pre>\\n\\n<p>How do I have to interpret the different ''improve values'' (0.02478172,0.02313676,...) and how are they related to the complexity parameter (0.02717093)?</p>\\n','\\n',char(10)", 9231, "2012-05-03 12:03:40.0", "How to interpret result from mvpart object in R?", "<r><interpretation><cart>", 0, 1, 930, "2012-05-03 12:03:40.0", "M C"]], "SELECT * FROM Table;": [[2243486, "2013-02-11 10:52:01.0", 49731, 15526, 1, "None"], [36761, 2, 8364, "None", "None", "2011-03-19"], [2388389, "2013-03-21 14:26:33.0", 52803, 34057, 1, "None"], [25496, "2011-01-04", 4450, "None", "None", 2]], "SELECT * FROM survey_responses;": [[22214, 2, 8330, "dde7ee59-1c8a-4280-92ba-e09b2cac9bf9", "2011-03-15 20:31:46.0", "I have a multiple response (categories) question (Q11, Q12, Q13, Q14, Q15).  My problem is that the data-entry clerck repeats the answers in the same case. Example\\\\n\\\\n           Q11 Q12 Q13 Q14 Q15\\\\nCase 1 001 003 015 001 022 (001 was type twice)\\\\n\\\\nCase 2 089 032 089 089 014 (089 was typed three times)\\\\n\\\\nI need to find a way to identified the cases with these errors.\\\\n\\\\nThanks,", "", "user3729"]], "SELECT * FROM Questions;": [[114172, 59582, 2, "Perhaps a more suitable title could be given to this question?", "2013-05-21 05:11:22.0", 24617]], "SELECT * FROM statistical_learning;": [[1159, "statistical-learning", 38]], "SELECT * FROM determinant;": [[1431, "determinant", 6]], "SELECT * FROM TableComments;": [[68270, 85, "Nice Question", "2014-02-25 07:11:38.0", "None", "None"], [70693, 35297, "eplace('@Bogdanovist You can just use latex markup.  I think what you''re looking for is \\n\\n$$\\n\\\\left[\\\\begin{array}{cc}\\n 1 & t \\\\\\\\\\n 0 & 1 \\\\\\\\\\n\\\\end{array}\\\\right]\\n$$\\n\\nwhich isi `$$\\n\\\\left[\\\\begin{array}{cc}\\n 1 & t \\\\\\\\\\n 0 & 1 \\\\\\\\\\n\\\\end{array}\\\\right]\\n$$`','\\n',char(10)", "2012-08-29 07:07:01.0", 1, 1739], [82386, 41804, "How is your approach different from Orges'' answer? Also the what if the assumption is not valid; plz see my update.", "2012-11-03 15:21:37.0", 0, 13473]], "SELECT * FROM logamadi;": [[40350, 7, "2014-02-16 06:50:34.0", "logamadi", "2014-03-30 17:37:44.0", 4, 0, 0, 3429933, "https://www.gravatar.com/avatar/b558bf7e090c40ec0455fdabb831b31e?s=128&d=identicon&r=PG&f=1"]], "SELECT * FROM musical-data-analysis;": [[1600, "musical-data-analysis", 4]], "SELECT * FROM Table:;": [[36964, 8526, 5, "2011-03-20", 1349, "None", "None", "None", "None", "None", "None"], [28746, 111323, 2, "2014-08-10 01:53:28.0", 3, "eplace('<p>A joint distribution has domain $(-\\\\infty, \\\\infty) \\\\times  (-\\\\infty, \\\\infty)$. If we partition each component of the cartesian product in two by selecting some value $x$ and some value $y$, then we get $4$ subsets,</p>\\n\\n<p>$$(-\\\\infty, x] \\\\times  (-\\\\infty, y],\\\\;\\\\;(-\\\\infty, x] \\\\times  [y,\\\\infty),\\\\\\\\\\n[x, \\\\infty) \\\\times  (-\\\\infty, y],\\\\;\\\\;[x, \\\\infty) \\\\times  [y,\\\\infty)$$</p>\\n\\n<p>made up of intersections of two events,</p>\\n\\n<p>$$A = P(X\\\\le x), \\\\;\\\\; B = P(Y\\\\le y)$$</p>\\n\\n<p>and their corresponding complements.</p>\\n\\n<p>Then (as the OP noted in a commnent),</p>\\n\\n<p>$$\\\\Pr(X\\\\ge x, Y\\\\ge y) = P(A^c\\\\cap B^c) = 1 - P(A\\\\cup B)$$</p>\\n\\n<p>$$=1-\\\\big[P(A) + P(B) - P(A\\\\cap B)\\\\big]$$</p>\\n\\n<p>So it appears that by taking the cross-partial derivative of $\\\\Pr(X\\\\ge x, Y\\\\ge y)$ we should again get the joint density. Let''s verify that:</p>\\n\\n<p>$$\\\\Pr(X\\\\ge x, Y\\\\ge y) = \\\\int_x^{\\\\infty}\\\\int_y^{\\\\infty}f(s,t)dtds$$</p>\\n\\n<p>$$\\\\frac {\\\\partial \\\\Pr(X\\\\ge x, Y\\\\ge y)}{\\\\partial y} = \\\\int_x^{\\\\infty} \\\\left(\\\\frac{\\\\partial}{\\\\partial y}\\\\int_y^{\\\\infty}f(s,t)dt\\\\right)ds $$</p>\\n\\n<p>$$=\\\\int_x^{\\\\infty}-f(s,y) ds$$</p>\\n\\n<p>$$\\\\frac {\\\\partial^2 \\\\Pr(X\\\\ge x, Y\\\\ge y)}{\\\\partial y\\\\partial x} = \\\\frac {\\\\partial }{\\\\partial x} \\\\int_x^{\\\\infty}-f(s,y) ds = -\\\\left(-f(x,y)\\\\right) = f(x,y)$$</p>\\n\\n<p>The above  also means that we can obtain the joint pdf from any of the four joint events indicated by the breakdown of the support -but in the other two cases, we should multiply by $-1$.</p>\\n\\n<p>$$\\\\begin{align} f(x,y) =&amp; \\\\frac {\\\\partial^2 \\\\Pr(X\\\\le x, Y\\\\le y)}{\\\\partial y\\\\partial x}\\\\\\\\\\n=&amp;\\\\frac {\\\\partial^2 \\\\Pr(X\\\\ge x, Y\\\\ge y)}{\\\\partial y\\\\partial x}\\\\\\\\\\n=&amp;-\\\\frac {\\\\partial^2 \\\\Pr(X\\\\le x, Y\\\\ge y)}{\\\\partial y\\\\partial x}\\\\\\\\\\n=&amp;-\\\\frac {\\\\partial^2 \\\\Pr(X\\\\ge x, Y\\\\le y)}{\\\\partial y\\\\partial x}\\n\\\\end{align}$$</p>\\n','\\n',char(10)", "2014-08-10 03:17:46.0", 2, 28746, "2014-08-10 03:17:46.0", 111317]], "SELECT * FROM \"Table;": [], "SELECT * FROM kernel_trick;": [[1258, "kernel-trick", 45]], "SELECT * FROM insert''),'\\\\,;": []}