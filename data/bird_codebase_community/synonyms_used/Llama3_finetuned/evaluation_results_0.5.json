{"SELECT * FROM PostsComments;": [[177986, 90942, 0, "+1. What''s the URL for the 3rd edition of Bayesian Data Analysis?", "2014-03-22 02:11:12.0", 24808, "None", "None", "None"], [82386, 41804, "None", "How is your approach different from Orges'' answer? Also the what if the assumption is not valid; plz see my update.", "2012-11-03 15:21:37.0", 13473, 0, "None", "None"], [97599, "None", "None", "How is this different from coding $X_2$ as zero for those who only attend the event?", "2014-05-14 01:18:36.0", 32036, 0, 190065, "None"], [114172, 59582, "None", "Perhaps a more suitable title could be given to this question?", "2013-05-21 05:11:22.0", 24617, 2, "None", "None"], [58210, "None", "None", "Your point about the number of penguins and chickens is interesting.  And I get what you''re saying.  However, the way that I read the question, the answer wasn''t intended to be so strongly dependent on the number of chickens or penguins.  What would your answer be if the number of chickens was equal to the number of penguins, and that number was large?", "2013-05-05 23:46:57.0", 25184, "None", 111764, 0]], "SELECT * FROM posts;": [[13737, 2, "2011-08-01 23:43:23.0", 3, "eplace('<p>It sounds like  you''re graphing a histogram.  Convert the y-axis from counts to proportion of the total.  Make sure that the range on the two graphs is the same after this conversion.</p>\\n','\\n',char(10)", 601, "2011-08-02 09:53:40.0", 2, 601, "2011-08-02 09:53:40.0", 13736]], "SELECT * FROM Tags;": [[290, "data-imputation", 127, 28270, 28269]], "SELECT * FROM PostLinksHistory;": [[3312222, "2014-08-21 17:00:44.0", 112705, 13875, 1]], "SELECT * FROM votes;": [[3563, 1114, 2, "2010-08-02", "None"], ["None", 4457, 2, "2010-12-09", 22583], [30228, 315, 2, "2011-02-10", "None"]], "SELECT * FROM tags;": [[1435, "pymc", 54, 46637, 46636, "None", "None", "None"], [1431, "determinant", 6, "None", "None", "None", "None", "None"], [1159, "statistical-learning", "None", "None", "None", 38, "None", "None"], ["None", "zero-inflated", "None", "None", "None", 1, 1869, "None"], [1600, "None", "None", "None", "None", 4, "None", "musical-data-analysis"], ["None", "None", "None", "None", "None", 45, 1258, "kernel-trick"]], "SELECT * FROM comments;": [[36777, 20393, 0, "Good answer. It is worth noting (though obvious) that working with a class of alternatives that is larger than $k$ for some small $k$ can often be computationally prohibitive, let alone if one has to work with an infinite or uncountable number of alternatives, which may also occur in practice. A big plus of the p-value approach is that it is often (usually?) computationally simple/tractable.", "2011-12-31 06:46:14.0", 7008, "None"], ["None", 35297, 1, "eplace('@Bogdanovist You can just use latex markup.  I think what you''re looking for is \\n\\n$$\\n\\\\left[\\\\begin{array}{cc}\\n 1 & t \\\\\\\\\\n 0 & 1 \\\\\\\\\\n\\\\end{array}\\\\right]\\n$$\\n\\nwhich isi `$$\\n\\\\left[\\\\begin{array}{cc}\\n 1 & t \\\\\\\\\\n 0 & 1 \\\\\\\\\\n\\\\end{array}\\\\right]\\n$$`','\\n',char(10)", "2012-08-29 07:07:01.0", 1739, 70693]], "SELECT * FROM Forecasts;": [[39597, 4, 13607, "7528cb3e-338e-4f2f-aea5-2164e69d1cc7", "2011-07-31 02:37:08.0", 183, "Can non-random samples be analyzed using standard statistical tests?", "edited tags; edited title", "", "None", "None", "None"], [353908, 2, "None", "64532f6b-b185-4393-8510-3d25fa07af7c", "2014-07-02 17:28:44.0", "None", "Cost for mis-classification can be used as below\\\\n\\\\nfit <- rpart(target ~ .,\\\\ndata=dataset, method=\"class\", parms=list( split=\"information\", loss=matrix(c(0,1,2,0), byrow=TRUE, nrow=2)), control=rpart.control(usesurrogate=0,maxsurrogate=0))\\\\n\\\\nIn the above example, I have considered that the cost of mis-classifying a -ve observation as 1 unit and cost of mis-classifying a +ve example as +ve as 2 units.", "None", "", 105562, 45985, ""]], "SELECT * FROM VoteLog;": [[27402, 6421, 2, "2011-01-21"]], "SELECT * FROM VotesHistory;": [[36761, 8364, 2, "2011-03-19"]], "SELECT * FROM postHistoryTable;": [[107915, 2, 33815, "ab296b9b-e71d-4140-ace4-9945f4dfccd6", "2012-08-07 02:57:20.0", 3262, "X and Y are not correlated (-.01); however, when I place X in a multiple regression predicting Y, alongside three other (related) variables, X and two other variables are significant predictors of Y. Note that the two other variables are significantly correlated with Y.\\\\n\\\\nHow should I interpret these findings? X predicts unique variance in Y, but since these are not correlated, it is somehow difficult to interpret. \\\\n\\\\nI know of opposite cases (i.e., two variables are correlated but regression is not significant) and those are relatively simpler to understand from a theoretical and statistical perspective.  \\\\n\\\\n\\\\n", "", ""]], "SELECT * FROM StackOverflowUsers;": [[40350, 7, "2014-02-16 06:50:34.0", "logamadi", "2014-03-30 17:37:44.0", 4, 0, 0, 3429933, "https://www.gravatar.com/avatar/b558bf7e090c40ec0455fdabb831b31e?s=128&d=identicon&r=PG&f=1"]], "SELECT * FROM Users;": [[588, 1034, "2010-07-29 10:36:25.0", "lokheart", "2014-09-05 16:19:18.0", 185, 38, 0, 156553, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [43721, 5, "None", "None", "2014-07-17 18:25:20.0", 1, "None", 0, 4321906, "2014-04-13 20:04:45.0", "user43721", 0, "https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1", "None", "None", "None", "None", "None", "None"], [48749, "None", "2014-06-20 21:26:13.0", "None", "2014-07-01 00:28:38.0", 0, 0, 0, 4642102, "None", "None", "None", "https://www.gravatar.com/avatar/769a4a9450283f19a6aaa33194038e1f?s=128&d=identicon&r=PG&f=1", 1, "user3761639", "None", "None", "None", "None"], [44222, "None", "2014-04-21 23:28:01.0", "None", "2014-06-08 01:00:17.0", 2, "None", 0, 4361372, "None", "user144794", "None", "https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1", 1, "None", 0, "None", "None", "None"], ["None", "None", "2011-03-20", "None", "None", "None", "None", "None", 1349, "None", "None", "None", "None", "None", "None", "None", 36964, 8526, 5]], "SELECT * FROM Badges;": [[55139, 20587, "Teacher", "2013-10-04 14:08:17.0", "None", "None", "None", "None"], ["None", 5594, "Teacher", "2011-09-23 13:19:21.0", 11416, "None", "None", "None"], ["None", 12053, "Supporter", "2012-07-13 14:40:15.0", 22946, "None", "None", "None"], [17690, 9912, "None", "2012-03-18 22:36:02.0", "None", "Scholar", "None", "None"], [25496, "None", "None", "2011-01-04", 4450, "None", 2, "None"], [68270, 85, "None", "2014-02-25 07:11:38.0", "None", "None", "None", "Nice Question"]], "SELECT * FROM PostLinkHistory;": [[533130, "2011-05-01 09:28:25.0", 10191, 5617, 1]], "SELECT * FROM users;": [[10616, 101, "2012-04-15 01:44:53.0", "Jonathan Callen", "2014-03-12 23:35:47.0", "http://example.com/", "United States", "eplace('<p>.</p>\\n','\\n',char(10)", 0, 0, 0, 913608, 27]], "SELECT * FROM postLinks;": [[2243486, "2013-02-11 10:52:01.0", 49731, 15526, 1, "None"], [2638298, "2013-09-13 15:28:32.0", 118, 3734, 1, "None"], [2495871, "2013-06-04 17:10:54.0", 60856, 44569, 1, "None"], [906579, "2011-08-05 06:07:07.0", "None", 13873, 1, 12495]], "SELECT * FROM Questions;": [[59455, 2, "2013-05-19 16:04:03.0", 0, "eplace('<p>A possible very practical approach could be calculate the ratio of the survival function of the distribution $\\\\Pr\\\\left(\\\\tilde X \\\\gt  1- \\\\alpha \\\\right)$ against the normal one, showing it is quite far greater. Another approach can be calculating the ratios of percentiles $w_1=\\\\frac{\\\\tilde{x_{99}}-\\\\tilde{x_{50}}}{\\\\tilde{x_{75}}-\\\\tilde{x_{50}}}$ of the distribution $\\\\tilde x$ under interest and dividing it against the normal one quantile values, $w_2=\\\\frac{\\\\tilde{\\\\Phi_{99}}-\\\\tilde{\\\\Phi_{50}}}{\\\\tilde{\\\\Phi_{75}}-\\\\tilde{\\\\Phi_{50}}}$, $\\\\tau=\\\\frac{w_1}{w_2}$.</p>\\n','\\n',char(10)", 6547, "2013-05-19 16:04:03.0", 0, 46889, "None", "None", "None", "None", "None", "None", "None", "None", "None"], [44620, 44616, "2012-11-28 17:15:34.0", 1, "eplace('<p>Could you use some sort of generalize additive model, where the dependent variables are relatedto the predictors as a smooth function, like done in the gam() function in R</p>\\n','\\n',char(10)", 8762, "2012-11-28 17:15:34.0", "None", "None", 2, 0, "None", "None", "None", "None", "None", "None", "None"], ["None", "None", "2014-08-18 17:38:46.0", 0, "eplace('<p>I am running a logistic regression model in R using multiply imputed data created using Amelia II, which I am then analyzing using Zelig. I would like to be able to report some measures of goodness-of-fit (e.g. likelihood ratio, pseudo R-squared, Hosmer-Lemeshow), however none are provided in the default Zelig output and I haven''t been able to figure out a way to extract any from the <code>zelig()</code> object. </p>\\n\\n<p>Do measures of goodness-of-fit need to be calculated differently when using multiply imputed datasets? Are there any R packages that are able to do this? I have looked into several packages that provide measures of goodness-of-fit, such as pscl, however they only work on glm objects, not MI objects created when using Amelia and Zelig.</p>\\n\\n<p>Thanks in advance for your help!</p>\\n','\\n',char(10)", 54277, "2014-08-30 12:36:07.0", 1, "None", 1, "None", 112348, 28, "Measures of goodness-of-fit using multiply imputed data in Zelig", "<r><regression><logistic><missing-data><multiple-imputation>", 1, 54277, "2014-08-18 18:24:35.0"]], "SELECT * FROM BadgesHistory;": [[81849, 48444, "Editor", "2014-06-16 13:45:59.0"]], "SELECT * FROM Posts;": [[16372, 2, "2011-10-02 23:11:12.0", 6, "eplace('<p>All three methods should produce the same result for the house edge.  To check yours, you should find </p>\\n\\n<pre><code>Number 1s     Prob\\n   0        125/216 \\n   1         75/216\\n   2         15/216\\n   3          1/216\\n</code></pre>\\n\\n<p>which would make your expected winnings (for a stake of 1) </p>\\n\\n<p>$$\\\\frac{-1\\\\times 125 + 1\\\\times 75 + 2 \\\\times 15 + 3 \\\\times 1}{216} = - \\\\frac{17}{216} \\\\approx -0.0787.$$</p>\\n\\n<p>You may need to check your calculations again.</p>\\n','\\n',char(10)", 2958, "2011-10-02 23:11:12.0", 1, 16365, "None", "None", "None", "None", "None", "None"], [111323, "None", "None", "None", "eplace('<p>A joint distribution has domain $(-\\\\infty, \\\\infty) \\\\times  (-\\\\infty, \\\\infty)$. If we partition each component of the cartesian product in two by selecting some value $x$ and some value $y$, then we get $4$ subsets,</p>\\n\\n<p>$$(-\\\\infty, x] \\\\times  (-\\\\infty, y],\\\\;\\\\;(-\\\\infty, x] \\\\times  [y,\\\\infty),\\\\\\\\\\n[x, \\\\infty) \\\\times  (-\\\\infty, y],\\\\;\\\\;[x, \\\\infty) \\\\times  [y,\\\\infty)$$</p>\\n\\n<p>made up of intersections of two events,</p>\\n\\n<p>$$A = P(X\\\\le x), \\\\;\\\\; B = P(Y\\\\le y)$$</p>\\n\\n<p>and their corresponding complements.</p>\\n\\n<p>Then (as the OP noted in a commnent),</p>\\n\\n<p>$$\\\\Pr(X\\\\ge x, Y\\\\ge y) = P(A^c\\\\cap B^c) = 1 - P(A\\\\cup B)$$</p>\\n\\n<p>$$=1-\\\\big[P(A) + P(B) - P(A\\\\cap B)\\\\big]$$</p>\\n\\n<p>So it appears that by taking the cross-partial derivative of $\\\\Pr(X\\\\ge x, Y\\\\ge y)$ we should again get the joint density. Let''s verify that:</p>\\n\\n<p>$$\\\\Pr(X\\\\ge x, Y\\\\ge y) = \\\\int_x^{\\\\infty}\\\\int_y^{\\\\infty}f(s,t)dtds$$</p>\\n\\n<p>$$\\\\frac {\\\\partial \\\\Pr(X\\\\ge x, Y\\\\ge y)}{\\\\partial y} = \\\\int_x^{\\\\infty} \\\\left(\\\\frac{\\\\partial}{\\\\partial y}\\\\int_y^{\\\\infty}f(s,t)dt\\\\right)ds $$</p>\\n\\n<p>$$=\\\\int_x^{\\\\infty}-f(s,y) ds$$</p>\\n\\n<p>$$\\\\frac {\\\\partial^2 \\\\Pr(X\\\\ge x, Y\\\\ge y)}{\\\\partial y\\\\partial x} = \\\\frac {\\\\partial }{\\\\partial x} \\\\int_x^{\\\\infty}-f(s,y) ds = -\\\\left(-f(x,y)\\\\right) = f(x,y)$$</p>\\n\\n<p>The above  also means that we can obtain the joint pdf from any of the four joint events indicated by the breakdown of the support -but in the other two cases, we should multiply by $-1$.</p>\\n\\n<p>$$\\\\begin{align} f(x,y) =&amp; \\\\frac {\\\\partial^2 \\\\Pr(X\\\\le x, Y\\\\le y)}{\\\\partial y\\\\partial x}\\\\\\\\\\n=&amp;\\\\frac {\\\\partial^2 \\\\Pr(X\\\\ge x, Y\\\\ge y)}{\\\\partial y\\\\partial x}\\\\\\\\\\n=&amp;-\\\\frac {\\\\partial^2 \\\\Pr(X\\\\le x, Y\\\\ge y)}{\\\\partial y\\\\partial x}\\\\\\\\\\n=&amp;-\\\\frac {\\\\partial^2 \\\\Pr(X\\\\ge x, Y\\\\le y)}{\\\\partial y\\\\partial x}\\n\\\\end{align}$$</p>\\n','\\n',char(10)", 28746, "2014-08-10 03:17:46.0", "None", 111317, 2, "2014-08-10 01:53:28.0", 3, 2, 28746, "2014-08-10 03:17:46.0"]], "SELECT * FROM ForecastsTable;": [[22214, 2, 8330, "dde7ee59-1c8a-4280-92ba-e09b2cac9bf9", "2011-03-15 20:31:46.0", "I have a multiple response (categories) question (Q11, Q12, Q13, Q14, Q15).  My problem is that the data-entry clerck repeats the answers in the same case. Example\\\\n\\\\n           Q11 Q12 Q13 Q14 Q15\\\\nCase 1 001 003 015 001 022 (001 was type twice)\\\\n\\\\nCase 2 089 032 089 089 014 (089 was typed three times)\\\\n\\\\nI need to find a way to identified the cases with these errors.\\\\n\\\\nThanks,", "", "user3729", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [27621, "None", "None", "None", "2012-05-03 11:20:49.0", "None", "None", "M C", 1, 1, 474, "eplace('<p>I used R <a href=\"http://cran.r-project.org/web/packages/mvpart/index.html\" rel=\"nofollow\">mvpart</a> package to create a multivariate regression tree. This is part of the output:</p>\\n\\n<pre><code>          CP               nsplit         rel error    xerror       xstd\\n1       0.02717093      0         1.0000000 1.0005358 0.03481409\\n2       0.01302184      2         0.9456581 0.9521266 0.03306820\\n\\nNode number 1: 3479 observations,    complexity param=0.02717093\\nMeans=12.94,0.5749,9.375,0.72,1.611,0.973,2.153,0.6209,3.307,3.702,2.422,0.3837,1.499,     Summed MSE=1305.19 \\nleft son=2 (992 obs) right son=3 (2487 obs)\\nPrimary splits:\\n  Dag           splits as  RRLLRRR, improve=0.02478172, (0 missing)\\n  Hoofdberoep    splits as  RLRLLRRLLLLRLL, improve=0.02313676, (0 missing)\\n  Ploegenstelsel splits as  RRLRRRL, improve=0.02191660, (0 missing)\\n  Werksituatie   splits as  LRLR, improve=0.02179270, (0 missing)\\n  Werkuren       splits as  RLRRRR, improve=0.02130351, (0 missing)\\n</code></pre>\\n\\n<p>How do I have to interpret the different ''improve values'' (0.02478172,0.02313676,...) and how are they related to the complexity parameter (0.02717093)?</p>\\n','\\n',char(10)", 9231, "2012-05-03 12:03:40.0", "How to interpret result from mvpart object in R?", "<r><interpretation><cart>", 0, 1, 930, "2012-05-03 12:03:40.0"]], "SELECT * FROM 1.0',;": [], "SELECT * FROM StatisticsHistoryTable;": [[21510, 5, 8104, "7072cab9-11d4-432e-a4c7-ae9af757f1db", "2011-03-10 04:37:35.0", 1381, "##Background\\\\n\\\\nOne of the most commonly used weak prior on variance is the inverse-gamma with parameters $\\\\alpha =0.001, \\\\beta=0.001$ [(Gelman 2006)][1].\\\\n\\\\nHowever, this distribution has a 90%CI of approximately $[3\\\\times10^{19},\\\\infty]$.\\\\n\\\\n    library(pscl)\\\\n    sapply(c(0.05, 0.95), function(x) qigamma(x, 0.001, 0.001))\\\\n\\\\n    [1] 3.362941e+19          Inf\\\\n\\\\nFrom this, I interpret that the $IG(0.001, 0.001)$ gives a low probability that variance will be very high, and the very low probability  that variance will be less than 1 $P(\\\\sigma<1|\\\\alpha=0.001, \\\\beta=0.001)=0.006$.\\\\n\\\\n    pigamma(1, 0.001, 0.001)\\\\n    [1] 0.006312353\\\\n\\\\n\\\\n##Question\\\\n\\\\nAm I missing something or is this actually an informative prior?\\\\n\\\\n*update* to clarify, the reason that I was considering this ''informative'' is because it claims very strongly that the variance is enormous and well beyond the scale of almost any variance ever measured. \\\\n\\\\n*follow-up* would a meta-analysis of a large number of variance estimates provide a more reasonable prior?\\\\n   \\\\n---\\\\n\\\\n##Reference\\\\n\\\\nGelman 2006. [Prior distributions for variance parameters in\\\\nhierarchical models][1]. Bayesian Analysis 1(3):515\u2013533\\\\n\\\\n[1]:http://www.stat.columbia.edu/~gelman/research/published/taumain.pdf", "added 320 characters in body", ""]], "SELECT * FROM postLinksHistoryTable;": [[2388389, "2013-03-21 14:26:33.0", 52803, 34057, 1]], "SELECT * FROM CommentsTable;": [[1886, 24, "2010-11-07 05:14:31.0", "fxnikee", "2010-11-07 05:14:31.0", "http://www.analyticstraining.in", "Bangalore, India", "eplace('<p>I am a Analytics consultant at REDWOOD ASSOCIATES. We also provide training on Analytics to students in India.\\nI am also a part time Forex Trader.</p>\\n','\\n',char(10)", 5, 0, 0, 509577, 26]], "SELECT * FROM Badges_History;": [[66518, 39707, "Supporter", "2014-02-07 22:31:27.0"]], "SELECT * FROM Table:;": [[129020, 2, 40979, "0f599233-264e-48db-83ce-7654889534b6", "2012-10-22 20:20:20.0", 919, "It appears that \"equitable\" means that the expected values of the team points, conditional on $n$, need to be equal regardless of team size.  This answer explores some consequences of this interpretation.\\\\n\\\\n-----------------------\\\\n\\\\nFor a team of size $n$ that is awarded $x(n)$ points if it \"wins\" and zero points otherwise, the expectation reduces simply to the probability of winning multiplied by $x(n)$.  With the independent \"fair coin\" model of choosing colors, wherein $n$ black values are found with probability $\\\\frac{1}{2}\\\\times \\\\frac{1}{2} \\\\times \\\\cdots \\\\times \\\\frac{1}{2} = 2^{-n}$, this expectation equals $x(n)2^{-n}$.  The condition $x(2)=10$ determines **the unique solution**\\\\n\\\\n$$x(n) = 10 \\\\times 2^{n-2}.$$\\\\n\\\\nNotice that this solution has nothing to do with $n$ being random or not.  However, the distribution of team points at the end of the \"competition\" will be strongly determined by the distribution of $n$. To explore this, **we can simulate a competition.**  (The code uses `R`.)\\\\n\\\\nFirst, we specify the number of teams `n.teams`, the number of trials during a competition `n.trials` (which is fixed and equal for all teams in this simulation), and the parameters for a random distribution of team sizes.  In this example, the distribution is Poisson (offset by $1$ to assure positive team sizes) with parameter `lambda`.  We also start the random number generator at a reproducible point.\\\\n\\\\n    lambda <- 3\\\\n    n.trials <- 100\\\\n    n.teams <- 10000\\\\n    set.seed(17)\\\\n\\\\nCreate the teams, assign hat colors to their members for each trial, count up the total number of wins, and award the points accordingly (using the solution $x(n)$):\\\\n\\\\n    n <- 1 + rpois(n.teams, lambda)\\\\n    n.black <- matrix(rbinom(n.trials * n.teams, size=n, p=1/2), nrow=n.teams) == n\\\\n    wins <- apply(n.black, 1, sum)\\\\n    points <- wins * sapply(n, function(n) 10 * 2^(n-2))\\\\n\\\\nHere is a plot of points *versus* team size in this simulation involving 100 trials for each of 10,000 independent teams (averaging `lambda+1` = $4$ people per team).  To make all the data visible, dots on the plot are randomly offset by up to 2.5% of the width and height of the plot.\\\\n\\\\n    jitter <- 0.025\\\\n    xr <- jitter*(range(n) - mean(n))\\\\n    yr <- jitter*(range(points) - mean(points))\\\\n    plot(n + runif(n.teams, xr[1], xr[2]), points + runif(n.teams, yr[1], yr[2]), \\\\n         xlab=\"Team size\", ylab=\"Total points\")\\\\n    abline(coef(lm(points ~ n)), lwd=2, col=\"Blue\", lty=2)\\\\n\\\\n![Plot][1]\\\\n\\\\nThe blue line is the least squares fit to the data.  Its horizontal trend attests to the \"egalitarian\" nature of the solution $x(n)$: the expected number of points is the same regardless of team size.  (In fact, inspecting the least squares fit with `summary(lm(points ~ n))` reveals it has an insignificant--and relatively small--slope.)\\\\n\\\\nPlease notice (a) the extremely skewed distribution of points and (b) the strong tendency for larger teams either to get no points at all or so many points that a single win assures a high standing at the end of the competition. The uniqueness of $x(n)$ shows that  *this behavior is forced on us by the requirements of the problem*.  It nicely illustrates the tradeoff between risk (exhibited as the large ranges of results for larger teams) and reward (apparent as the numbers of points).\\\\n\\\\nIt is instructive to re-run the simulation with different values of the input. When `lambda` grows, for instance, most teams are relatively large, so only a few--and not necessarily the largest--account for almost all the points.\\\\n\\\\n  [1]: http://i.stack.imgur.com/EtMBW.png", "", ""]]}