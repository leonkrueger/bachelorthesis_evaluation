{"SELECT * FROM user_actions_log_2014_02_15_19_17_55_0_0_0_0_0_0_0;": [[40350, 7, "2014-02-16 06:50:34.0", "logamadi", "2014-03-30 17:37:44.0", 4, 0, 0, 3429933, "https://www.gravatar.com/avatar/b558bf7e090c40ec0455fdabb831b31e?s=128&d=identicon&r=PG&f=1"]], "SELECT * FROM transactions_invoices_table_1_0_0_0_0_0_0_0_0_0_0_0_0;": [[1435, "pymc", 54, 46637, 46636]], "SELECT * FROM user_activity_log_history_status_history_status_history_status_history_status_history_status_history_status_history_status_history_status_history_status_history_status_history_status_history_status_history_status_history;": [[48749, 1, "2014-06-20 21:26:13.0", "user3761639", "2014-07-01 00:28:38.0", 0, 0, 0, 4642102, "https://www.gravatar.com/avatar/769a4a9450283f19a6aaa33194038e1f?s=128&d=identicon&r=PG&f=1"]], "SELECT * FROM post_comments_2011_1_26_1459304_0_0_0_0_0_0_0_0_;": [[82386, 41804, 0, "How is your approach different from Orges'' answer? Also the what if the assumption is not valid; plz see my update.", "2012-11-03 15:21:37.0", 13473]], "SELECT * FROM transactions_2013_9_13_15_28_32_0_0_0_0_0_0_0_0;": [[2638298, "2013-09-13 15:28:32.0", 118, 3734, 1]], "SELECT * FROM order_status_history_logistics_warehouse_inventory_table_1_0_1_0_0_0_0_0_0_0_;": [[906579, "2011-08-05 06:07:07.0", 13873, 12495, 1]], "SELECT * FROM payment_event_streaming_history_transaction_status_history_table_4_5_2017_2_17_9_14_23_PM_UTC_;": [[30228, 315, 2, "2011-02-10"]], "SELECT * FROM paper;journal;author_id;year;month;day;hour;minute;second;title;doi;journal_id;": [[129020, 2, 40979, "0f599233-264e-48db-83ce-7654889534b6", "2012-10-22 20:20:20.0", 919, "It appears that \"equitable\" means that the expected values of the team points, conditional on $n$, need to be equal regardless of team size.  This answer explores some consequences of this interpretation.\\\\n\\\\n-----------------------\\\\n\\\\nFor a team of size $n$ that is awarded $x(n)$ points if it \"wins\" and zero points otherwise, the expectation reduces simply to the probability of winning multiplied by $x(n)$.  With the independent \"fair coin\" model of choosing colors, wherein $n$ black values are found with probability $\\\\frac{1}{2}\\\\times \\\\frac{1}{2} \\\\times \\\\cdots \\\\times \\\\frac{1}{2} = 2^{-n}$, this expectation equals $x(n)2^{-n}$.  The condition $x(2)=10$ determines **the unique solution**\\\\n\\\\n$$x(n) = 10 \\\\times 2^{n-2}.$$\\\\n\\\\nNotice that this solution has nothing to do with $n$ being random or not.  However, the distribution of team points at the end of the \"competition\" will be strongly determined by the distribution of $n$. To explore this, **we can simulate a competition.**  (The code uses `R`.)\\\\n\\\\nFirst, we specify the number of teams `n.teams`, the number of trials during a competition `n.trials` (which is fixed and equal for all teams in this simulation), and the parameters for a random distribution of team sizes.  In this example, the distribution is Poisson (offset by $1$ to assure positive team sizes) with parameter `lambda`.  We also start the random number generator at a reproducible point.\\\\n\\\\n    lambda <- 3\\\\n    n.trials <- 100\\\\n    n.teams <- 10000\\\\n    set.seed(17)\\\\n\\\\nCreate the teams, assign hat colors to their members for each trial, count up the total number of wins, and award the points accordingly (using the solution $x(n)$):\\\\n\\\\n    n <- 1 + rpois(n.teams, lambda)\\\\n    n.black <- matrix(rbinom(n.trials * n.teams, size=n, p=1/2), nrow=n.teams) == n\\\\n    wins <- apply(n.black, 1, sum)\\\\n    points <- wins * sapply(n, function(n) 10 * 2^(n-2))\\\\n\\\\nHere is a plot of points *versus* team size in this simulation involving 100 trials for each of 10,000 independent teams (averaging `lambda+1` = $4$ people per team).  To make all the data visible, dots on the plot are randomly offset by up to 2.5% of the width and height of the plot.\\\\n\\\\n    jitter <- 0.025\\\\n    xr <- jitter*(range(n) - mean(n))\\\\n    yr <- jitter*(range(points) - mean(points))\\\\n    plot(n + runif(n.teams, xr[1], xr[2]), points + runif(n.teams, yr[1], yr[2]), \\\\n         xlab=\"Team size\", ylab=\"Total points\")\\\\n    abline(coef(lm(points ~ n)), lwd=2, col=\"Blue\", lty=2)\\\\n\\\\n![Plot][1]\\\\n\\\\nThe blue line is the least squares fit to the data.  Its horizontal trend attests to the \"egalitarian\" nature of the solution $x(n)$: the expected number of points is the same regardless of team size.  (In fact, inspecting the least squares fit with `summary(lm(points ~ n))` reveals it has an insignificant--and relatively small--slope.)\\\\n\\\\nPlease notice (a) the extremely skewed distribution of points and (b) the strong tendency for larger teams either to get no points at all or so many points that a single win assures a high standing at the end of the competition. The uniqueness of $x(n)$ shows that  *this behavior is forced on us by the requirements of the problem*.  It nicely illustrates the tradeoff between risk (exhibited as the large ranges of results for larger teams) and reward (apparent as the numbers of points).\\\\n\\\\nIt is instructive to re-run the simulation with different values of the input. When `lambda` grows, for instance, most teams are relatively large, so only a few--and not necessarily the largest--account for almost all the points.\\\\n\\\\n  [1]: http://i.stack.imgur.com/EtMBW.png", "", ""]], "SELECT * FROM Competition_Results_2012_2013_2014_2015_2016_2017_2018_2019_202;": [[36761, 8364, 2, "2011-03-19"]], "SELECT * FROM competition_data_archive_2014_2016_2017_2018_2019_2020_2021_2022_202;": [[2388389, "2013-03-21 14:26:33.0", 52803, 34057, 1]], "SELECT * FROM purchase_order_details_table_2_0_0_0_0_0_0_0_0_0_0_0_0;": [[3312222, "2014-08-21 17:00:44.0", 112705, 13875, 1]], "SELECT * FROM paper_paper_judges_in_zelig_with_multiple_imputation_in_r_using_amelia_ii_and_zelig_in_r_language_and_statistics_language_and_statistics;": [[112348, 1, "2014-08-18 17:38:46.0", 0, 28, "eplace('<p>I am running a logistic regression model in R using multiply imputed data created using Amelia II, which I am then analyzing using Zelig. I would like to be able to report some measures of goodness-of-fit (e.g. likelihood ratio, pseudo R-squared, Hosmer-Lemeshow), however none are provided in the default Zelig output and I haven''t been able to figure out a way to extract any from the <code>zelig()</code> object. </p>\\n\\n<p>Do measures of goodness-of-fit need to be calculated differently when using multiply imputed datasets? Are there any R packages that are able to do this? I have looked into several packages that provide measures of goodness-of-fit, such as pscl, however they only work on glm objects, not MI objects created when using Amelia and Zelig.</p>\\n\\n<p>Thanks in advance for your help!</p>\\n','\\n',char(10)", 54277, "2014-08-30 12:36:07.0", "Measures of goodness-of-fit using multiply imputed data in Zelig", "<r><regression><logistic><missing-data><multiple-imputation>", 1, 1, 54277, "2014-08-18 18:24:35.0"]], "SELECT * FROM answer_question_in_tutorial_on_logistic_regression_with_amelia_II_and_zelig_in_R;answer_question_in_tutorial_on_logistic_regression_with;": [[114172, 59582, 2, "Perhaps a more suitable title could be given to this question?", "2013-05-21 05:11:22.0", 24617]], "SELECT * FROM answers_in_zelig_logistic_regression_on_multiple_imputed_data_in_R_using_amelia_II_and_zelig_package_jstor_2013_201;": [[36777, 20393, 0, "Good answer. It is worth noting (though obvious) that working with a class of alternatives that is larger than $k$ for some small $k$ can often be computationally prohibitive, let alone if one has to work with an infinite or uncountable number of alternatives, which may also occur in practice. A big plus of the p-value approach is that it is often (usually?) computationally simple/tractable.", "2011-12-31 06:46:14.0", 7008]], "SELECT * FROM logamadi;7;2014-02-16_06:50:34.0;logamadi;2014-03-;": [[70693, 35297, 1, "eplace('@Bogdanovist You can just use latex markup.  I think what you''re looking for is \\n\\n$$\\n\\\\left[\\\\begin{array}{cc}\\n 1 & t \\\\\\\\\\n 0 & 1 \\\\\\\\\\n\\\\end{array}\\\\right]\\n$$\\n\\nwhich isi `$$\\n\\\\left[\\\\begin{array}{cc}\\n 1 & t \\\\\\\\\\n 0 & 1 \\\\\\\\\\n\\\\end{array}\\\\right]\\n$$`','\\n',char(10)", "2012-08-29 07:07:01.0", 1739]], "SELECT * FROM Table:_point_assignment_in_logamadi_competition_database_2014_2_16_6_50_34_0;;": [[1258, "kernel-trick", 45]], "SELECT * FROM logistic_regression_in_zelig_database_table_2_0_0_0_0_0_0_0_0_0_0;": [[2243486, "2013-02-11 10:52:01.0", 49731, 15526, 1]], "SELECT * FROM logistic_regression_in_r_database_table3;logistic_regression_in_r_database_table4;logistic_regression_in_r_database_table5;logistic_regression;": [[353908, 2, 105562, "64532f6b-b185-4393-8510-3d25fa07af7c", "2014-07-02 17:28:44.0", 45985, "Cost for mis-classification can be used as below\\\\n\\\\nfit <- rpart(target ~ .,\\\\ndata=dataset, method=\"class\", parms=list( split=\"information\", loss=matrix(c(0,1,2,0), byrow=TRUE, nrow=2)), control=rpart.control(usesurrogate=0,maxsurrogate=0))\\\\n\\\\nIn the above example, I have considered that the cost of mis-classifying a -ve observation as 1 unit and cost of mis-classifying a +ve example as +ve as 2 units.", "", ""]], "SELECT * FROM logistic_regression_model_in_R_using_multiply_imputed_data_created_by_Amelia_II;;": [[39597, 4, 13607, "7528cb3e-338e-4f2f-aea5-2164e69d1cc7", "2011-07-31 02:37:08.0", 183, "Can non-random samples be analyzed using standard statistical tests?", "edited tags; edited title", ""]], "SELECT * FROM logistics_regression_in_zelig_r_package_jagran_newspaper_group;logistics_regression_in_zelig_r_package_jagran_newspaper;": [[107915, 2, 33815, "ab296b9b-e71d-4140-ace4-9945f4dfccd6", "2012-08-07 02:57:20.0", 3262, "X and Y are not correlated (-.01); however, when I place X in a multiple regression predicting Y, alongside three other (related) variables, X and two other variables are significant predictors of Y. Note that the two other variables are significantly correlated with Y.\\\\n\\\\nHow should I interpret these findings? X predicts unique variance in Y, but since these are not correlated, it is somehow difficult to interpret. \\\\n\\\\nI know of opposite cases (i.e., two variables are correlated but regression is not significant) and those are relatively simpler to understand from a theoretical and statistical perspective.  \\\\n\\\\n\\\\n", "", ""]], "SELECT * FROM log_entries_2014_02_25_07_11_38_0;;": [[68270, 85, "Nice Question", "2014-02-25 07:11:38.0"]], "SELECT * FROM logamadi;7;2014-03-30_17:37:44.0;4;0;0;3429933;": [[21510, 5, 8104, "7072cab9-11d4-432e-a4c7-ae9af757f1db", "2011-03-10 04:37:35.0", 1381, "##Background\\\\n\\\\nOne of the most commonly used weak prior on variance is the inverse-gamma with parameters $\\\\alpha =0.001, \\\\beta=0.001$ [(Gelman 2006)][1].\\\\n\\\\nHowever, this distribution has a 90%CI of approximately $[3\\\\times10^{19},\\\\infty]$.\\\\n\\\\n    library(pscl)\\\\n    sapply(c(0.05, 0.95), function(x) qigamma(x, 0.001, 0.001))\\\\n\\\\n    [1] 3.362941e+19          Inf\\\\n\\\\nFrom this, I interpret that the $IG(0.001, 0.001)$ gives a low probability that variance will be very high, and the very low probability  that variance will be less than 1 $P(\\\\sigma<1|\\\\alpha=0.001, \\\\beta=0.001)=0.006$.\\\\n\\\\n    pigamma(1, 0.001, 0.001)\\\\n    [1] 0.006312353\\\\n\\\\n\\\\n##Question\\\\n\\\\nAm I missing something or is this actually an informative prior?\\\\n\\\\n*update* to clarify, the reason that I was considering this ''informative'' is because it claims very strongly that the variance is enormous and well beyond the scale of almost any variance ever measured. \\\\n\\\\n*follow-up* would a meta-analysis of a large number of variance estimates provide a more reasonable prior?\\\\n   \\\\n---\\\\n\\\\n##Reference\\\\n\\\\nGelman 2006. [Prior distributions for variance parameters in\\\\nhierarchical models][1]. Bayesian Analysis 1(3):515\u2013533\\\\n\\\\n[1]:http://www.stat.columbia.edu/~gelman/research/published/taumain.pdf", "added 320 characters in body", ""]], "SELECT * FROM logistic_regression_on_multiply_imputed_data_in_zelig;logistic_regression;logistic_regression;logistic_regression;logistic_regression;logistic;": [[1600, "musical-data-analysis", 4]], "SELECT * FROM Journalist;Editor;2014-06-16_13:45:59.0;4;0;0;3429933;;": [[81849, 48444, "Editor", "2014-06-16 13:45:59.0"]], "SELECT * FROM logamadi;2014-03-30_17:37:44.0;4;0;3429933;https://www;": [[22946, 12053, "Supporter", "2012-07-13 14:40:15.0"]], "SELECT * FROM 48749;1;2014-06-20_21:26:13.0;user3761639;2014-07-;": [[27402, 6421, 2, "2011-01-21"]], "SELECT * FROM log_data;;": [[25496, 4450, 2, "2011-01-04"]], "SELECT * FROM Teacher;2011-09-23_13:19:21.0;1;1;0;0;0;0;0;": [[11416, 5594, "Teacher", "2011-09-23 13:19:21.0"]], "SELECT * FROM logistic_regression_on_multiply_imputed_data_in_zelig_table2_table4_table3_table1_table6_table5_table7_table8_table9_table;": [[44620, 2, "2012-11-28 17:15:34.0", 1, "eplace('<p>Could you use some sort of generalize additive model, where the dependent variables are relatedto the predictors as a smooth function, like done in the gam() function in R</p>\\n','\\n',char(10)", 8762, "2012-11-28 17:15:34.0", 0, 44616]], "SELECT * FROM StatisticalLearning;38;38;38;38;38;38;38;38;38;38;38;38;38;;": [[1159, "statistical-learning", 38]], "SELECT * FROM win_points_table_2014_8_21_17_0_44_0_0_0_4642102_https_www;": [[2495871, "2013-06-04 17:10:54.0", 60856, 44569, 1]], "SELECT * FROM predict:_journal;": [[111323, 2, "2014-08-10 01:53:28.0", 3, "eplace('<p>A joint distribution has domain $(-\\\\infty, \\\\infty) \\\\times  (-\\\\infty, \\\\infty)$. If we partition each component of the cartesian product in two by selecting some value $x$ and some value $y$, then we get $4$ subsets,</p>\\n\\n<p>$$(-\\\\infty, x] \\\\times  (-\\\\infty, y],\\\\;\\\\;(-\\\\infty, x] \\\\times  [y,\\\\infty),\\\\\\\\\\n[x, \\\\infty) \\\\times  (-\\\\infty, y],\\\\;\\\\;[x, \\\\infty) \\\\times  [y,\\\\infty)$$</p>\\n\\n<p>made up of intersections of two events,</p>\\n\\n<p>$$A = P(X\\\\le x), \\\\;\\\\; B = P(Y\\\\le y)$$</p>\\n\\n<p>and their corresponding complements.</p>\\n\\n<p>Then (as the OP noted in a commnent),</p>\\n\\n<p>$$\\\\Pr(X\\\\ge x, Y\\\\ge y) = P(A^c\\\\cap B^c) = 1 - P(A\\\\cup B)$$</p>\\n\\n<p>$$=1-\\\\big[P(A) + P(B) - P(A\\\\cap B)\\\\big]$$</p>\\n\\n<p>So it appears that by taking the cross-partial derivative of $\\\\Pr(X\\\\ge x, Y\\\\ge y)$ we should again get the joint density. Let''s verify that:</p>\\n\\n<p>$$\\\\Pr(X\\\\ge x, Y\\\\ge y) = \\\\int_x^{\\\\infty}\\\\int_y^{\\\\infty}f(s,t)dtds$$</p>\\n\\n<p>$$\\\\frac {\\\\partial \\\\Pr(X\\\\ge x, Y\\\\ge y)}{\\\\partial y} = \\\\int_x^{\\\\infty} \\\\left(\\\\frac{\\\\partial}{\\\\partial y}\\\\int_y^{\\\\infty}f(s,t)dt\\\\right)ds $$</p>\\n\\n<p>$$=\\\\int_x^{\\\\infty}-f(s,y) ds$$</p>\\n\\n<p>$$\\\\frac {\\\\partial^2 \\\\Pr(X\\\\ge x, Y\\\\ge y)}{\\\\partial y\\\\partial x} = \\\\frac {\\\\partial }{\\\\partial x} \\\\int_x^{\\\\infty}-f(s,y) ds = -\\\\left(-f(x,y)\\\\right) = f(x,y)$$</p>\\n\\n<p>The above  also means that we can obtain the joint pdf from any of the four joint events indicated by the breakdown of the support -but in the other two cases, we should multiply by $-1$.</p>\\n\\n<p>$$\\\\begin{align} f(x,y) =&amp; \\\\frac {\\\\partial^2 \\\\Pr(X\\\\le x, Y\\\\le y)}{\\\\partial y\\\\partial x}\\\\\\\\\\n=&amp;\\\\frac {\\\\partial^2 \\\\Pr(X\\\\ge x, Y\\\\ge y)}{\\\\partial y\\\\partial x}\\\\\\\\\\n=&amp;-\\\\frac {\\\\partial^2 \\\\Pr(X\\\\le x, Y\\\\ge y)}{\\\\partial y\\\\partial x}\\\\\\\\\\n=&amp;-\\\\frac {\\\\partial^2 \\\\Pr(X\\\\ge x, Y\\\\le y)}{\\\\partial y\\\\partial x}\\n\\\\end{align}$$</p>\\n','\\n',char(10)", 28746, "2014-08-10 03:17:46.0", 2, 28746, "2014-08-10 03:17:46.0", 111317]], "SELECT * FROM Predicted_Table:_user3761639;": [[1886, 24, "2010-11-07 05:14:31.0", "fxnikee", "2010-11-07 05:14:31.0", "http://www.analyticstraining.in", "Bangalore, India", "eplace('<p>I am a Analytics consultant at REDWOOD ASSOCIATES. We also provide training on Analytics to students in India.\\nI am also a part time Forex Trader.</p>\\n','\\n',char(10)", 5, 0, 0, 509577, 26]], "SELECT * FROM predict:_points;": [[36964, 8526, 5, "2011-03-20", 1349]], "SELECT * FROM logamadi;2014-03-30_17:37:44.0;4;0;0;3429933;https;": [[22583, 4457, 2, "2010-12-09", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [290, 28270, "data-imputation", 127, 28269, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [10616, 0, 101, "2012-04-15 01:44:53.0", "Jonathan Callen", "2014-03-12 23:35:47.0", "http://example.com/", "United States", "eplace('<p>.</p>\\n','\\n',char(10)", 0, 0, 913608, 27, "None", "None", "None", "None", "None"], [588, "None", "None", "2010-07-29 10:36:25.0", 1034, "2014-09-05 16:19:18.0", "None", "None", "lokheart", 0, 156553, 38, 185, "None", "None", "None", "None", "None"], [17690, "None", "None", "None", 9912, "2012-03-18 22:36:02.0", "None", "None", "Scholar", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [3563, "None", 2, "2010-08-02", 1114, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [16372, "None", 2, "2011-10-02 23:11:12.0", 1, "2011-10-02 23:11:12.0", 16365, "None", "eplace('<p>All three methods should produce the same result for the house edge.  To check yours, you should find </p>\\n\\n<pre><code>Number 1s     Prob\\n   0        125/216 \\n   1         75/216\\n   2         15/216\\n   3          1/216\\n</code></pre>\\n\\n<p>which would make your expected winnings (for a stake of 1) </p>\\n\\n<p>$$\\\\frac{-1\\\\times 125 + 1\\\\times 75 + 2 \\\\times 15 + 3 \\\\times 1}{216} = - \\\\frac{17}{216} \\\\approx -0.0787.$$</p>\\n\\n<p>You may need to check your calculations again.</p>\\n','\\n',char(10)", 6, "None", 2958, "None", "None", "None", "None", "None", "None"], [177986, "None", "None", "None", 90942, "2014-03-22 02:11:12.0", "None", "None", "+1. What''s the URL for the 3rd edition of Bayesian Data Analysis?", 0, "None", 24808, "None", "None", "None", "None", "None", "None"], [27621, 9231, 1, "2012-05-03 11:20:49.0", 1, "2012-05-03 12:03:40.0", 0, "None", "eplace('<p>I used R <a href=\"http://cran.r-project.org/web/packages/mvpart/index.html\" rel=\"nofollow\">mvpart</a> package to create a multivariate regression tree. This is part of the output:</p>\\n\\n<pre><code>          CP               nsplit         rel error    xerror       xstd\\n1       0.02717093      0         1.0000000 1.0005358 0.03481409\\n2       0.01302184      2         0.9456581 0.9521266 0.03306820\\n\\nNode number 1: 3479 observations,    complexity param=0.02717093\\nMeans=12.94,0.5749,9.375,0.72,1.611,0.973,2.153,0.6209,3.307,3.702,2.422,0.3837,1.499,     Summed MSE=1305.19 \\nleft son=2 (992 obs) right son=3 (2487 obs)\\nPrimary splits:\\n  Dag           splits as  RRLLRRR, improve=0.02478172, (0 missing)\\n  Hoofdberoep    splits as  RLRLLRRLLLLRLL, improve=0.02313676, (0 missing)\\n  Ploegenstelsel splits as  RRLRRRL, improve=0.02191660, (0 missing)\\n  Werksituatie   splits as  LRLR, improve=0.02179270, (0 missing)\\n  Werkuren       splits as  RLRRRR, improve=0.02130351, (0 missing)\\n</code></pre>\\n\\n<p>How do I have to interpret the different ''improve values'' (0.02478172,0.02313676,...) and how are they related to the complexity parameter (0.02717093)?</p>\\n','\\n',char(10)", 1, 930, 474, "None", "How to interpret result from mvpart object in R?", "<r><interpretation><cart>", "2012-05-03 12:03:40.0", "M C", "None"], [43721, "None", 5, "None", "user43721", "2014-04-13 20:04:45.0", 4321906, "None", "https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1", 0, 0, 1, "None", "None", "None", "2014-07-17 18:25:20.0", "None", "None"], [22214, "", 2, "None", "None", "None", "None", "None", 8330, "None", "None", "None", "None", "I have a multiple response (categories) question (Q11, Q12, Q13, Q14, Q15).  My problem is that the data-entry clerck repeats the answers in the same case. Example\\\\n\\\\n           Q11 Q12 Q13 Q14 Q15\\\\nCase 1 001 003 015 001 022 (001 was type twice)\\\\n\\\\nCase 2 089 032 089 089 014 (089 was typed three times)\\\\n\\\\nI need to find a way to identified the cases with these errors.\\\\n\\\\nThanks,", "None", "2011-03-15 20:31:46.0", "user3729", "dde7ee59-1c8a-4280-92ba-e09b2cac9bf9"], [111764, "None", "None", "None", 58210, 25184, "None", "None", "Your point about the number of penguins and chickens is interesting.  And I get what you''re saying.  However, the way that I read the question, the answer wasn''t intended to be so strongly dependent on the number of chickens or penguins.  What would your answer be if the number of chickens was equal to the number of penguins, and that number was large?", 0, "None", "None", "None", "None", "None", "2013-05-05 23:46:57.0", "None", "None"], [59455, "None", 2, "None", "None", "2013-05-19 16:04:03.0", "None", "None", "eplace('<p>A possible very practical approach could be calculate the ratio of the survival function of the distribution $\\\\Pr\\\\left(\\\\tilde X \\\\gt  1- \\\\alpha \\\\right)$ against the normal one, showing it is quite far greater. Another approach can be calculating the ratios of percentiles $w_1=\\\\frac{\\\\tilde{x_{99}}-\\\\tilde{x_{50}}}{\\\\tilde{x_{75}}-\\\\tilde{x_{50}}}$ of the distribution $\\\\tilde x$ under interest and dividing it against the normal one quantile values, $w_2=\\\\frac{\\\\tilde{\\\\Phi_{99}}-\\\\tilde{\\\\Phi_{50}}}{\\\\tilde{\\\\Phi_{75}}-\\\\tilde{\\\\Phi_{50}}}$, $\\\\tau=\\\\frac{w_1}{w_2}$.</p>\\n','\\n',char(10)", 0, 6547, 46889, 0, "None", "None", "2013-05-19 16:04:03.0", "None", "None"], [1869, "None", 1, "None", "None", "None", "None", "None", "zero-inflated", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [1431, "None", "None", "None", "None", "None", "None", "None", "determinant", "None", "None", "None", "None", 6, "None", "None", "None", "None"], [533130, "None", 10191, "2011-05-01 09:28:25.0", "None", "None", 5617, "None", "None", "None", "None", 1, "None", "None", "None", "None", "None", "None"], [66518, "None", "None", "None", 39707, "2014-02-07 22:31:27.0", "None", "None", "Supporter", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [55139, "None", "None", "None", 20587, "2013-10-04 14:08:17.0", "None", "None", "Teacher", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [190065, "None", 97599, "None", "None", "2014-05-14 01:18:36.0", "None", "None", "How is this different from coding $X_2$ as zero for those who only attend the event?", 0, "None", 32036, "None", "None", "None", "None", "None", "None"]], "SELECT * FROM p_value_logistic_regression_on_multiply_imputed_data_in_zelig_package_r_language;;": [[13737, 2, "2011-08-01 23:43:23.0", 3, "eplace('<p>It sounds like  you''re graphing a histogram.  Convert the y-axis from counts to proportion of the total.  Make sure that the range on the two graphs is the same after this conversion.</p>\\n','\\n',char(10)", 601, "2011-08-02 09:53:40.0", 2, 601, "2011-08-02 09:53:40.0", 13736]], "SELECT * FROM logistic_regression_with_poisson_error;logistic_regression_with_poisson_error;logistic_regression_with_poisson_error;logistic_regression;": [[44222, 1, "2014-04-21 23:28:01.0", "user144794", "2014-06-08 01:00:17.0", 2, 0, 0, 4361372, "https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1"]], "SELECT * FROM The_Best',1',0;1',1',1',1',1',1',1.1.1.0',1.0;": []}