{"SELECT * FROM Comments;": [[144377, 74147, 0, "Updated as requested.", "2013-10-30 17:40:08.0", 31481], [215256, 112308, 0, "If the url is wrong, email the author! he is the only one who can do something with it", "2014-08-18 14:19:12.0", 11887], [152376, 77857, 2, "Pepto, your question in the comment there goes far beyond clarification, to a substantive additional question. It''s a bit much to ask Elvis for one-on-one responses; you should instead address such questions to the whole site - that is, you should either edit your original question to reflect what you meant to post to begin with, or post a new question (with, if necessary, a link back to this one).", "2013-11-27 17:42:20.0", 805], [197090, 101365, 0, "Your primary question seems statistical, so I don''t think you need R in the title, but you may want it among your tags.", "2014-06-06 06:54:13.0", 805], [108876, 56463, 0, "@phil12 you can also just specify the `seasonal.periods`, and `tbats` will choose the best specification for each of the other parameters.  Also, your data''s biggest period is clearly larger than 7-- I would try specifying `seasonal.periods=265`, AFTER trying to remove the spikes.", "2013-04-18 16:39:33.0", 2817], [188951, 96923, 0, "thanks a lot @gung", "2014-05-08 22:56:05.0", 31474], [184138, 94266, 0, "Yes, indeed. With probability.", "2014-04-18 18:37:53.0", 6244], [7719, 5148, 0, "The purpose of only considering the tail subsequence of the original samples is because there is time needed to wait for the instrumental markov chain to have its distribution converge to its stationary distribution, whose density has been designed to be the target function.", "2010-12-04 23:32:34.0", 1005]], "SELECT * FROM postHistory;": [[60979, 5, 20087, "8890f85b-9d3f-4c52-8c24-86c16cb82162", "2011-12-21 14:19:14.0", 3919, "[Assuming Weibull is appropriate] Johnson Kotz and Balakrishnan''s book has a lot of ways to estimate Weibull parameters.  Some of these do not depend on the data not including zeroes (e.g. using the mean and standard deviation, or using certain percentiles).\\\\n\\\\nJohnson, N. L., Kotz, S., and Balakrishnan, N. (1994). Continuous Univariate Distributions. New York: Wiley, roughly on page 632.", "added 129 characters in body", ""], [3271, 6, 866, "e2f483ab-2f28-47dd-9b65-285d3dde5d52", "2010-08-09 12:25:13.0", 159, "<regression><lasso><ridge-regression>", "edited tags", ""], [221266, 5, 67233, "4ada5d31-ad65-4823-bc0b-914b07c96c06", "2013-08-13 16:19:03.0", 29087, "If you only have the four data points, I think the best way to do this is with a G^2 test. You want to start by assuming the frequency is a binomial distribution (every person in the population has the condition with probability p). And your null hypothesis is that p_1=p_2=p_3=p_4. \\\\n\\\\nSo the overall mean is (1800+539+490+301)/(2.9m+1.327m+.88m+.268m)=0.000582.\\\\n\\\\nYour expected cases in each group are 1688.7, 772.7, 512.4, and 156.1. You can calculate the G^2 statistic, but the answer I get is 192.8, which is chi-squared(3) under the null hypothesis. This is a very low p-value, so you''d reject the null and say that yes, you can be quite confident that the incidence is different between these locations. \\\\n\\\\nIn particular that last location is considerably higher than the other three, so that is contributing heavily to the low p-value. You can repeat this analysis for the other three and you may get something a bit different, but that is an exercise to the reader :-)\\\\n\\\\nHTH\\\\n\\\\nETA: the DF is 3, not 1, as Yves pointed out in the comments.", "added 65 characters in body", ""], [157835, 4, 49675, "9e99103d-5b0d-4a49-b476-e1efaf286c42", "2013-02-10 12:01:52.0", 686, "Probability of a specific element in different sets having the same attribute value", "fixed typo", ""], [17004, 13, 6652, "e3bf70cf-6f3b-489b-b347-3a6765fd77e6", "2011-01-28 01:35:26.0", 1347, "{\"Voters\":[{\"Id\":1347,\"DisplayName\":\"dsimcha\"}]}", "", ""], [72516, 1, 23648, "df139d4f-b12f-4f8f-9e36-1919927cf011", "2012-02-25 06:14:44.0", 6946, "Estimating a 1-D Brownian motion process using noisy observations", "", ""], [24696, 2, 9143, "2d826dd0-444c-4e3b-8e22-0f397acfeb2f", "2011-04-04 01:20:36.0", 2958, "You cannot be certain without knowing the distribution.  But there are certain things you can do, such as looking at what might be called the \"partial variance\", i.e. if you have a sample of size $N$, you draw the variance estimated from the first $n$ terms, with $n$ running from 2 to $N$.\\\\n\\\\nWith a finite population variance, you hope that the partial variance soon settles down close to the population variance.\\\\n\\\\nWith an infinite population variance, you see jumps up in the partial variance followed by slow declines until the next very large value appears in the sample.\\\\n\\\\nThis is an illustration with Normal and Cauchy random variables (and a log scale)\\\\n![Partial Variance][1]  \\\\n\\\\nThis may not help if the shape of your distribution is such that a much larger sample size than you have is needed to identify it with sufficient confidence, i.e. where very large values are fairly (but not extremely) rare for a distribution with finite variance, or are extremely rare for a distribution with infinite variance.  For a given distribution there will be sample sizes which are more likely than not to reveal its nature; conversely, for a given sample size, there are distributions which are more likely than not to disguise their natures for that size of sample.  \\\\n\\\\n\\\\n  [1]: http://i.stack.imgur.com/P2iOf.png", "", ""], [339018, 2, 100732, "0a1f9a4f-4aa0-49da-9b85-c0eb6b115872", "2014-06-01 00:03:48.0", 44764, "Assuming you meant a binomial likelihood,\\\\n\\\\n$$\\\\n\\\\begin{eqnarray*}\\\\n\\\\text{Posterior}(\\\\theta) & \\\\propto & \\\\text{Likelihood}(\\\\theta) \\\\times \\\\text{Prior}(\\\\theta) \\\\\\\\\\\\n& = & \\\\text{Binomial}(20 \\\\mid 30, \\\\theta) \\\\times \\\\bigg[ \\\\lambda \\\\times \\\\text{Beta}(\\\\theta \\\\mid 20,10) + (1-\\\\lambda) \\\\times \\\\text{Beta}(\\\\theta \\\\mid 20, 20) \\\\bigg] \\\\\\\\\\\\n& = & \\\\lambda \\\\times \\\\text{Binomial}(20 \\\\mid 30, \\\\theta) \\\\times \\\\text{Beta}(\\\\theta \\\\mid 20,10) \\\\\\\\\\\\n&+& (1 - \\\\lambda) \\\\times \\\\text{Binomial}(20 \\\\mid 30, \\\\theta) \\\\times \\\\text{Beta}(\\\\theta \\\\mid 20,20) \\\\\\\\ \\\\\\\\\\\\n& = & \\\\lambda { 30 \\\\choose 20} \\\\frac{1}{\\\\text{B}(20, 10)} \\\\theta^{40 - 1} (1-\\\\theta)^{20-1} \\\\\\\\\\\\n&+& (1-\\\\lambda) {30 \\\\choose 20} \\\\frac{1}{\\\\text{B}(20,20)} \\\\theta^{40-1} (1-\\\\theta)^{30-1} \\\\\\\\ \\\\\\\\\\\\n& = & \\\\lambda { 30 \\\\choose 20} \\\\frac{\\\\text{B}(40,20) \\\\text{B}(40,30)}{\\\\text{B}(20, 10) \\\\text{B}(40,20) \\\\text{B}(40,30)} \\\\theta^{40 - 1} (1-\\\\theta)^{20-1} \\\\\\\\\\\\n&+& (1-\\\\lambda) {30 \\\\choose 20} \\\\frac{\\\\text{B}(40,20) \\\\text{B}(40,30)}{\\\\text{B}(20,20) \\\\text{B}(40,20) \\\\text{B}(40,30)} \\\\theta^{40-1} (1-\\\\theta)^{30-1} \\\\\\\\ \\\\\\\\\\\\n& = & \\\\lambda { 30 \\\\choose 20} \\\\frac{ \\\\text{B}(40,20)}{\\\\text{B}(20, 10)} \\\\text{Beta}(\\\\theta \\\\mid 40,20) \\\\\\\\\\\\n& + & (1- \\\\lambda) { 30 \\\\choose 20} \\\\frac{\\\\text{B}(40,30)}{\\\\text{B}(20, 20)} \\\\text{Beta}(\\\\theta \\\\mid 40,30) \\\\\\\\ \\\\\\\\\\\\n& \\\\propto & \\\\lambda \\\\frac{ \\\\text{B}(40,20)}{\\\\text{B}(20, 10)} \\\\text{Beta}(\\\\theta \\\\mid 40,20) \\\\\\\\\\\\n& + & (1- \\\\lambda) \\\\frac{\\\\text{B}(40,30)}{\\\\text{B}(20, 20)} \\\\text{Beta}(\\\\theta \\\\mid 40,30).\\\\n\\\\end{eqnarray*}\\\\n$$\\\\nThus, the new weights $\\\\omega_1, \\\\omega_2$ are\\\\n$$\\\\n\\\\begin{eqnarray*}\\\\n\\\\omega_1 & = & \\\\left( \\\\lambda \\\\frac{ \\\\text{B}(40,20)}{\\\\text{B}(20, 10)} \\\\right) \\\\left( \\\\lambda \\\\frac{ \\\\text{B}(40,20)}{\\\\text{B}(20, 10)} + (1- \\\\lambda) \\\\frac{\\\\text{B}(40,30)}{\\\\text{B}(20, 20)} \\\\right)^{-1} \\\\\\\\\\\\n\\\\omega_2 & = & 1 - \\\\omega_1,\\\\n\\\\end{eqnarray*}\\\\n$$\\\\nand\\\\n$$\\\\n\\\\text{Posterior}(\\\\theta) = \\\\omega_1 \\\\times \\\\text{Beta}(\\\\theta \\\\mid 40,20) + \\\\omega_2 \\\\times \\\\text{Beta}(\\\\theta \\\\mid 40,30).\\\\n$$", "", ""], [269223, 5, 80628, "3376188d-c7e4-4902-b2ec-aa0f86eaf156", "2013-12-27 13:36:56.0", 31420, "How can I derive the distribution of $$\\\\bar{X^2}\\\\quad  \\\\text{when}\\\\quad X\\\\sim N \\\\left( \\\\theta, \\\\sigma^2 \\\\right) $$\\\\n\\\\nThe context of this question is an exercise requiring me to show that $\\\\bar{X^2}- \\\\frac{\\\\sigma^2}{n} $ is an unbiased estimator of $\\\\theta^2$ and afterwards find its efficiency. Thus I must find its variance as well. \\\\n\\\\nObviously $\\\\bar{X} \\\\sim N \\\\left(\\\\theta ,\\\\frac{\\\\sigma^2}{n} \\\\right) $ but when it comes to its square I would not know.\\\\n\\\\nI suppose that my only option is to derive the distribution of $\\\\bar{X^2}$ using the CDF technique and repeatedly use change of variables. I tried that but it leads to a mess. I cannot discern the resulting distribution either\\\\n\\\\nIs there perhaps a neater way to derive the distribution of $\\\\bar{X^2}$? Thanks.", "deleted 156 characters in body; edited title", ""], [7429, 3, 3100, "22ab45bf-8340-48a3-bac8-8ff49851bc3a", "2010-09-27 06:24:37.0", 183, "<regression><curve-fitting><nonlinear-regression>", "", ""]], "SELECT * FROM postLinks;": [[2515792, "2013-06-19 17:54:29.0", 62106, 61711, 1], [2994127, "2014-04-20 02:30:29.0", 94412, 16921, 1], [2759962, "2013-12-03 07:21:23.0", 78354, 63639, 1], [1623199, "2012-06-03 21:00:00.0", 29731, 29563, 1], [2732360, "2013-11-13 20:16:39.0", 76498, 50272, 1], [1108423, "2011-09-16 10:30:38.0", 15621, 577, 1], [1673729, "2012-07-05 12:20:33.0", 31679, 24681, 1], [1946191, "2012-11-28 11:45:49.0", 30526, 44586, 1], [1629457, "2012-06-07 19:53:05.0", 30013, 29906, 1]], "SELECT * FROM badges;": [[12806, 319, "Good Answer", "2011-11-07 17:57:28.0"], [18111, 5921, "Popular Question", "2012-03-28 17:05:32.0"], [91523, 1739, "Nice Answer", "2014-09-05 17:29:23.0"], [35731, 19676, "Editor", "2013-01-15 07:42:07.0"], [54998, 17614, "Supporter", "2013-10-02 18:08:58.0"], [64299, 35326, "Student", "2014-01-27 12:26:19.0"], [79593, 46244, "Supporter", "2014-06-03 14:30:54.0"], [8103, 449, "Good Answer", "2011-05-21 16:23:42.0"], [9954, 5672, "Student", "2011-08-03 23:38:06.0"], [16257, 9267, "Editor", "2012-02-16 22:43:53.0"]], "SELECT * FROM Posts;": [[359, "standard-error", 350, 28667, 28666, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [205255, "None", "None", "None", "None", 1, 63359, "9ee57dd5-0ef4-4083-ab07-b3e44668cb97", "2013-07-04 17:55:43.0", 8653, "Trade-off between using explicit inputs and using fewer inputs for neural networks", "", "", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [207841, "None", "None", "None", "None", 5, 63994, "c86c888a-ce50-4109-b1be-c1fbd9bd6913", "2013-07-11 09:58:43.0", 805, "As I said in comments, you could work out (via simulation at the very least) a distribution for the difference in sample mean and sample median, which would be symmetric about 0 and whose variance multiplied by n would asymptotically go to some constant. As such you could construct some kind of test for normality, but it would be a pretty poor test for it, since it would have fairly poor power against a host of symmetric alternatives that aren''t normal. If you''re interested in assessing normality, there are better ways.\\\\n\\\\nTo answer the question though, [this paper](http://siba-ese.unisalento.it/index.php/ejasa/article/view/11468/11411) says that asymptotically, that constant I mentioned is $\\\\pi/2-1$ (that is the variance of $\\\\bar x - \\\\tilde{x}$ in large samples is about $0.571\\\\sigma^2/n$. In small samples, it''s a bit smaller. As a rough rule of thumb, you expect the standard deviation of the difference between mean and median to be about $0.75 \\\\sigma/\\\\sqrt{n}$ (in odd samples; a bit smaller for even $n$). \\\\n\\\\nSimulation of 10000 samples of size 25 gives a constant of $0.7390$ (that is, the s.d. of the difference was about $0.739\\\\sigma/\\\\sqrt{n}$, which is consistent with the results from the paper.\\\\n\\\\nThis boils down to basically using [Pearson''s second skewness coefficient](http://en.wikipedia.org/wiki/Skewness#Pearson.27s_skewness_coefficients) as a way of assessing normality. \\\\n\\\\nNow, considering it as a test statistic, since $\\\\sigma$ will generally be unknown, it must usually be estimated; except for large samples (when we may apply Slutsky''s theorem), this will lead to a coefficient that''s heavier tailed than normal - though not actually t-distributed, it will probably be close to it\\\\* - meaning a critical value will tend to be larger for smaller samples. This will somewhat counteract the effect above of the coefficient being smaller with smaller $n$.\\\\n\\\\n\\\\* though we won''t know suitable approximate df without further effort\\\\n\\\\n", "added 627 characters in body", "", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [49804, "None", "None", "None", "None", 2, 16746, "bde5925b-3754-4618-8670-b5e6379f5201", "2011-10-09 14:21:39.0", 364, "There is no clear answer in the literature on how to deal with the fact that RT data do not conform to the traditional Gaussian models of error. Some folks will say that one need not worry about non-Gaussian error within cells of the experimental design because traditional analysis approaches typically collapse these observations to a mean, and we know that the sampling distribution of the mean will tend to conform to the Gaussian assumption of error. Such arguments miss the point, now well established in the RT literature, that experimental variables can affect not only the central tendency of the RT distribution but also the scale and shape of the distribution. Indeed, a number of reports show that the traditional \"collapse to a mean\" approach can miss phenomena when a variable decreases the central tendency but increases skew.\\\\n\\\\nFor those interested in characterizing the full nature of the RT distribution (or at least, features of the distribution beyond central tendency), there are three general approaches:\\\\n\\\\n 1. Quantify the distribution via quantiles (typically `seq(.1,.9,.2)`) and add quantile to the list of fixed effect variables in your analysis. This requires that you have some method for dealing with the continuous-yet-likely-nonlinear nature of quantile as a variable; I like generalized additive mixed effects modelling for this purpose.\\\\n 2. Choose an a priori distribution form (ex. Ex-Gaussian, Wald, Weibull, etc) and attempt to estimate the best fitting parameters of this distribution (and the effect of your experimental variables thereon) given the data. While some folks will obtain parameter estimates per cell of the experimental design then submit the estimates to ANOVA, but where the assumption of Gaussian error may not be appropriate for these parameter estimates, I''d say that hierachircal modelling (as in [Rouder et al 2005][1]) is a better approach.\\\\n 3. Choose an a priori process model (ex. diffusion, linear ballistic accumulator, etc) and fit it repeatedly to the data, comparing the quality of the resulting fits as a function of whether you let certain parameters of interest to vary as a function of the experimental design. Again, this is best done in a hierarchical manner.\\\\n\\\\nPersonally, I like #3 but suggest that #1 should always be done as well to guard against the possibility that the process model isn''t appropriate.\\\\n\\\\nNow, how to do any of the above and account for outliers (fast and slow) is another matter, though Trisha Van Zandt had a neat talk at the SCiP meeting last year where she showed an approach where fast and slow outliers were explicitly modelled with a priori distributions (her modelling also accounted for serial correlations in RTs nicely). \\\\n\\\\n\\\\n  [1]: http://pcl.missouri.edu/apps?q=node/36", "", "", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [94872, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 1, 94876, "2014-04-23 14:39:15.0", 7, 325, "eplace('<p>Is the claim that functions of independent random variables are themselves independent, true? </p>\\n\\n<p>I have seen that result often used implicitly in some proofs, for example in the proof of independence between the sample mean and the sample variance of a normal distribution, but I have not been able to find justification for it. It seems that some authors take it as given but I am not certain that this is always the case.</p>\\n\\n<p>Thank you.</p>\\n','\\n',char(10)", 31420, "2014-04-24 17:40:09.0", "Functions of Independent Random Variables", "<probability><self-study><random-variable><independence>", 2, 0, 4, 31420, "2014-04-24 09:42:40.0", "None"], [364252, "None", "None", "None", "None", 3, 108911, "dd088d71-ca81-47a5-a9e8-eecfc5c70e4d", "2014-07-22 20:06:58.0", 17054, "<hypothesis-testing><frequentist>", "", "", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [94454, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 1, 94456, "2014-04-20 15:31:05.0", -2, 77, "eplace('<p>Will any dataset clustered via each of the following methods:  </p>\\n\\n<ul>\\n<li>Agglomerative Hierarchical Clustering using \"complete linkage\" method</li>\\n<li>Agglomerative Hierarchical Clustering using \"single linkage\" method</li>\\n</ul>\\n\\n<p>have the same dendrogram structure?</p>\\n\\n<p>If yes, please help prove it or show a contradicting example (if not).</p>\\n','\\n',char(10)", 44139, "2014-04-20 18:42:04.0", "Agglomerative Hierarchical Clustering \"complete linkage\" as opposed to \"single linkage\" dendrogram", "<clustering><hierarchical>", 3, 4, 0, 22047, "2014-04-20 15:35:33.0", "None"], [45205, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 2, "None", "2012-12-05 16:53:14.0", 1, "None", "eplace('<p>This is a <em>classification problem</em>. Machine learning has a lot of tools to address problems such as these, e.g., Neural Networks, Support Vector Machines (SVM), Classification and Regression Trees (CART) etc.</p>\\n','\\n',char(10)", 1352, "2012-12-05 16:53:14.0", "None", "None", "None", 0, "None", "None", "None", 45122], [311165, "None", "None", "None", "None", 2, 92461, "b39cb4ed-b401-43af-a440-6b45a879eaa2", "2014-04-03 20:44:47.0", 4505, "A lot of this depends on what question(s) you are trying to answer.\\\\n\\\\nThe 3rd model with the quasibinomial family, I don''t know what question it really answers.  Smarter people than me have spent more time than me trying to figure that out and have not worked it out yet (yes it gives an answer, but what does it mean?) so I tend to avoid the quasi families.\\\\n\\\\nModel 2 is the logistic regression, it simplifies the data by just looking at if the value if greater than or less than 10000, in the process it throws out much of the detail and so may not give as much information.  It will treat a diamond with price 9999 exactly the same as one with price 1, but differently from one that is 10001 (even though that is only a difference of 2), but it will not be affected by nonlinearity in the carat/price relationship that could affect the linear model.  Which is really more important to you?\\\\n\\\\nThe final model with the poisson family is appropriate when the values of price are discrete (you can have a price of 1 or 2, but not 1.5 or other values between) and non-negative, which is probably the case here.  But for a Poisson with a large mean the distribution can be well approximated by a normal distribution, so you probably will not see a lot of difference between the linear model and the Poisson model unless the variability is much different.  The Poisson model (with default options) does posit a relationship between the mean and the variance and a certain form of non-linearity, so it could gain you there.\\\\n\\\\nWhat is most important are the assumptions you are willing to make based on the science behind the data (not the data itself) and what questions your are trying to answer.", "", "", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [23782, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 2, "None", "2012-02-28 05:08:52.0", 3, "None", "eplace('<p>yes there are statistics tests you can use to do the comparison between the two experiments. This answer assumes you have paired data, i.e. the same people or animals or transistors or whatever were used in both experiments.</p>\\n\\n<p>Assuming your data is not normally distributed, or there is a relatively small number of unique counts per experiment, you could use the <a href=\"http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test\" rel=\"nofollow\">Wilcoxon signed rank test</a>. Alternatively, if your data are reasonably normally distributed and you have a relatively large range of counts, you could use a <a href=\"http://en.wikipedia.org/wiki/Student%27s_t-test\" rel=\"nofollow\">paired t-test</a> instead. You will be able to do these tests in the statistical software of your choice.</p>\\n\\n<p>If the results from the two experiments are statistically significantly different, that suggests that the change between the two is affecting the results.</p>\\n','\\n',char(10)", 8605, "2012-02-28 05:08:52.0", "None", "None", "None", 0, "None", "None", "None", 23771], [87018, "None", "None", "None", "None", "None", 44832, "None", "2012-12-01 03:53:34.0", 7549, "Yeah, this is what I went with. To give a bit more detail, I''m trying to use a weighted logistic regressor as the ''weak learner'' in an AdaBoost classifier. So I need some way of adjusting the classifier based on the AdaBoost weights. However, I''m not yet completely sure that just putting a linear weight on the log likelihood as I''ve proposed is the correct way to minimize the total misclassification error, which is what AdaBoost wants.", "None", "None", "None", "None", "None", 0, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [58441, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 2, "None", "2013-05-08 06:11:46.0", 0, "None", "eplace('<p>Your dependent variable is nationality? I''m not really sure that makes sense. Your DV should be something you want to explain/predict/describe using the covariates. Sounds like you want a model where vacation length is the DV and is explained by nationality.</p>\\n\\n<p>More generally, you can use results from this table to interpret direction and statistical significance of coefficients, but not to speak about their absolute size or size relative to the effects of other covariates. You''ll have to do some addition analyses (e.g., showing predicted probabilities for different combinations of covariate values) to say something more specific about size of these relationships.</p>\\n','\\n',char(10)", 25138, "2013-05-08 06:11:46.0", "None", "None", "None", 0, "None", "None", "None", 58415], [15487, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 1, "None", "2011-09-12 20:45:38.0", 1, 399, "eplace('<p>Is there any justfification for producing a standard error of a single exponentially weighted coefficient?</p>\\n\\n<p>If yes, how can we interpret the p-value?</p>\\n\\n<hr>\\n\\n<p><strong>Background</strong></p>\\n\\n<p>I use SAS ETS to estimate a single exponential smoothing model. When i select the optimize option for the estimation of the smoothing weight I get the estimated value (optimal), a standard error, a t-value and a p-value. How is the standard error calculated (e.g. what is the formula for it) and what is the menaing of the p-value. In the users guide it says that: </p>\\n\\n<blockquote>\\n  <p>The standard errors associated with the smoothing weights are\\n  calculated from the Hessian matrix of the sum of squared,\\n  one-step-ahead prediction errors with respect to the smoothing weights\\n  used in the optimization process.</p>\\n</blockquote>\\n\\n<p>As far as I know the optimal weight is calcualted with a non-linear optimization procedure (heuristcs search with objective function to be minimzed the MSE or somthing similar). Based on this the statistical concepts are not applicable in this case (e.g. if we repeat the calculations in another sample then in the 95% of the cases etc.) So what is the meaning of the p-value and the standard error calcualtion? Do we have to check whether the smoothing coefficient is significant or not or we just disregard the standard error and the p-values? </p>\\n\\n<p>Is there any use of the standard error, the t-value and the p-value?</p>\\n','\\n',char(10)", 6281, "2011-09-13 12:32:55.0", "Standard error and p-values of exponential smoothing weights", "<exponential-smoothing>", 1, 0, "None", 1036, "2011-09-13 12:32:55.0", "None"], [87534, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 2, "None", "2014-02-22 21:53:49.0", 2, "None", "eplace('<p>The general formula is\\n$$ P(A_1 \\\\cup \\\\cdots \\\\cup A_n) = \\\\sum_{k=1}^n (-1)^{k+1} \\\\sum_{i \\\\in C_{k,n}} P(A_{i_1}\\\\cap \\\\cdots \\\\cap A_{i_k}) $$\\nwhere $C_{k,n}$ is the set of all ordered $k$-uples $i_1 &lt; \\\\cdots &lt; i_k$ of $\\\\{1,\\\\dots,n\\\\}$.</p>\\n\\n<p>You can prove it by induction, it\u2019s not conceptually difficult but painful enough to write.</p>\\n','\\n',char(10)", 8076, "2014-02-22 21:53:49.0", "None", "None", "None", 0, "None", "None", "None", 87533], [78572, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 1, "None", "2013-12-04 18:42:44.0", 0, 39, "eplace('<p>10 species were sampled over 5 time periods (put simply, growth is the measured variable).</p>\\n\\n<p>Now I''d like to find out:</p>\\n\\n<ol>\\n<li><p>Do the measured values significantly differ between species within each time period (both concerning mean and trend)?</p></li>\\n<li><p>Do the measured values significantly differ on species-level over the different time periods?</p></li>\\n</ol>\\n\\n<p>(My objective: Can I group species without losing information?)</p>\\n\\n<p>How can I test this?</p>\\n\\n<p>What''s more, for each species several individuals were sampled and sometimes multiple measurements within one individual were done. Ideally, I would like to end up with one mean value for each species, but I guess I first have to find out wether there are significant differences between individuals/within individuals? Which test can I use for that purpose? And do I proceed from broad to specific (within species -> within individuals) or from specific to broad (within individuals -> within species)? </p>\\n\\n<p>I am not experienced with statistical analysis at all and would appreciate any hint on how to proceed.</p>\\n','\\n',char(10)", 34927, "2013-12-04 19:01:39.0", "Which statistical test(s) do I use? (Groups, time series)", "<group-differences>", 0, 0, "None", 22047, "2013-12-04 19:01:39.0", "None"], [97077, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 1, "None", "2014-05-09 15:07:17.0", 0, 82, "eplace('<p>I trained two svms (LIBSVM) with 15451 samples after I did a 10-fold cross-validation and found the best parameter values for gamma and C (RBF kernel). In one svm I used just 1 feature and in the second an additional one (to see whether this additional is improving prediction). After CV I have am accuracy of 75 % (SVM with one feature) and 77 % (SVM with that additional one). After testing on another 15451 instances I have an accuracy of 70 % and 72 % respectively.</p>\\n\\n<p>I know that this is called overfitting but is it significant here, since it is only a difference of 5 %.</p>\\n\\n<p>What could I do to avoid overfitting?</p>\\n\\n<p>Is it even good to use just one or two features and a relatively big training set?</p>\\n\\n<p>Hope you can help me out.</p>\\n','\\n',char(10)", 44892, "2014-05-09 15:07:17.0", "LIBSVM overfitting", "<svm><cross-validation><libsvm><overfitting>", 0, 2, "None", "None", "None", "None"], [196157, "None", "None", "None", "None", "None", 100898, "None", "2014-06-02 17:12:48.0", 46059, "I have already done that before and I am getting some weird results. See my edited question.", "None", "None", "None", "None", "None", 0, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None"], [7332, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 2, "None", "2011-02-17 14:41:10.0", 6, "None", "eplace('<p>The answer, I believe, to your first question is \"yes\". This can be accomplished with <a href=\"http://en.wikipedia.org/wiki/Givens_rotation\" rel=\"nofollow\">Givens rotations</a>, which allow for the annihilation of arbitrary elements of a matrix via a $2\\\\times 2$ rotation matrix. The implication is that if you start with a rotation matrix, then you can reduce it to a diagonal matrix via Givens rotations. But, since orthogonality of a matrix is preserved by multiplication with another orthogonal matrix, this means that the diagonal matrix must be orthogonal, and hence, must contain only 1''s and -1''s. Additional rotations then reduce this matrix to the identity. The affirmative answer to your first question follows immediately. </p>\\n\\n<p>Thus the space of $n\\\\times n$ orthogonal matrices is spanned by Givens rotations with respect to matrix multiplication. </p>\\n\\n<p>If this doesn''t give you enough detail, let me know and I''ll fill it in. </p>\\n','\\n',char(10)", 2970, "2011-02-17 19:44:10.0", "None", "None", "None", 5, "None", 449, "2011-02-17 19:44:10.0", 7330]], "SELECT * FROM tags;": [[1502, "mice", 20, 55697, 55696], [389, "point-process", 42, "None", "None"], [219, "cross-validation", 614, 28325, 28324], [216, "sas", 331, 19816, 19815], [1661, "ggplot", 1, 73890, 73889], [843, "queueing", 29, "None", "None"], [1371, "arithmetic", 35, "None", "None"], [1186, "gumbel", 8, "None", "None"]], "SELECT * FROM comments;": [[187999, 96417, 0, "@Alexis Perhaps we are thinking about things differently, but I don''t think that''s any reason to respond rudely. Why don''t you try giving an example to illustrate your point instead of acting argumentative and shocked that I could espouse such a view.", "2014-05-05 16:29:06.0", 44451], [54677, 28949, 0, "The odds ratio in a $2 \\\\times 2$ table is $p_{11}p_{00}/p_{01}p_{10}$ where $p_{ij}$ are the cell probabilities. If the $2 \\\\times 2$ table is composed of the same items with itself, all of the elements will reside in either the $(1,1)$ or $(0,0)$ cell. Therefore the odds ratio is $1/0 = \\\\infty$. I don''t know what $x/x = 1$ has to do with it.", "2012-05-22 19:12:02.0", 4856], [63544, 32384, 0, "Also to note dials are frequently used for other instruments on the dashboard, but don''t go in the same direction! See the picture of the dial for engine temp in the dashboard in the Robert Kosara post I link to.", "2012-07-16 23:19:59.0", 1036], [126016, 65252, 0, "Do you really, actually only have four vectors, or is this a simplification (ie, how large are the groups in reality?)", "2013-07-23 13:24:14.0", 18459], [114707, 59825, 0, "Sorry, my keyboard wasn''t cooperating, I placed the reply by accident. See the previous , edited, post.", "2013-05-23 13:33:49.0", 26005]], "SELECT * FROM Users;": [[9011, 31, "2012-02-07 06:20:50.0", "Eamonn Keogh", "2012-04-07 23:29:42.0", "http://www.cs.ucr.edu/~eamonn/", 48, 0, 0, 1228990, "None", "None", "None", "None", "None", "None", "None"], [13139, 6, "2012-08-07 16:45:03.0", "srk", "2012-08-09 12:47:25.0", "None", 0, 0, 0, 1728412, "None", "None", "None", "None", "None", "None", "None"], [7756, 16, "2011-12-03 01:12:59.0", "Keivan", "2011-12-22 00:05:53.0", "None", 4, 0, 0, 1081621, "None", "None", "None", "None", "None", "None", "None"], [35529, 1, "2013-12-01 12:09:12.0", "user112626", "2014-05-07 22:08:42.0", "None", 0, 0, 0, 3665862, "https://www.gravatar.com/avatar/bfc32a91c38691b4ddaa471bb8eb31b2?s=128&d=identicon&r=PG&f=1", "None", "None", "None", "None", "None", "None"], [53898, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 30592, "Student", "2013-09-23 11:07:00.0", "None", "None", "None"], [54919, 126, "2014-08-29 14:18:19.0", "jdj081", "2014-09-04 18:20:00.0", "None", 0, 0, 0, 2552490, "https://www.gravatar.com/avatar/042783eecdb931c4302377724bcc34dc?s=128&d=identicon&r=PG", "None", "None", "None", "Alabama", "eplace('<p>I have a BS and and MS degree in physics, and I am working as an engineer. I have always enjoyed programming, and currently my favorite language is Python.</p>\\n','\\n',char(10)", 34], [27850, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 14525, "Student", "2012-10-01 12:58:14.0", "None", "None", "None"], [45128, "None", "None", "None", "None", "None", "None", "None", "None", "None", "None", 25908, "Editor", "2013-05-21 06:10:29.0", "None", "None", "None"], [6307, 101, "2011-09-14 14:44:26.0", "Gunnar Sj\u00far\u00f0arson Knudsen", "2014-09-12 13:18:00.0", "None", 1, 10, 0, 257879, "https://www.gravatar.com/avatar/a124dbbfc75bc65b56b9dabebd55e77e?s=128&d=identicon&r=PG&f=1", "None", "None", "None", "Faroe Islands", "eplace('<p>Currently Data-miner / Business Intelligence developer at Insurance Company and undergrad Comp.sci. student in the Faroe Islands.</p>\\n\\n<p>Previously studied actuarial mathematics in Copenhagen.</p>\\n\\n<p>Especially interested in machine-learning algorithms and data-warehouse structures.</p>\\n','\\n',char(10)", 23], [49435, 1, "2014-07-03 13:37:11.0", "user49435", "2014-07-03 13:37:11.0", "None", 0, 0, 0, 4697347, "https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1", "None", "None", "None", "None", "None", "None"]], "SELECT * FROM posts;": [[19865, 2, "2011-12-15 14:11:44.0", 4, "eplace('<p>The way that you''ve envisioned the analysis is really not the way I would suggest you start out thinking about it.  First of all it is easy to show that if cutoffs <em>must</em> be used, cutoffs are not applied on individual features but on the overall predicted probability.  The optimal cutoff for a single covariate depends on all the levels of the other covariates; it cannot be constant.  Secondly, ROC curves play no role in meeting the goal of making optimum decisions for an <em>individual</em> subject.</p>\\n\\n<p>To handle correlated scales there are many data reduction techniques that can help.  One of them is a formal redundancy analysis where each predictor is nonlinearly predicted from all the other predictors, in turn.  This is implemented in the <code>redun</code> function in the R <code>Hmisc</code> package.  Variable clustering, principal component analysis, and factor analysis are other possibilities.  But the main part of the analysis, in my view, should be building a good probability model (e.g., binary logistic model).</p>\\n','\\n',char(10)", 4253, "2011-12-15 14:11:44.0", 1, 3259, "None", "None", "None", "None", "None", "None", "None"], [58354, 1, "2013-05-07 12:04:36.0", 0, "eplace('<p>My dataset consists of 4,000 companies from 24 countries. Each company has certain features, the IV''s. Consider the variable list below: green is the DV, red the IV''s and blue are dummies (one for each country a company might be from) for which I want to control the OLS output. There are 24 countries entered.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/KLz3z.png\" alt=\"enter image description here\"></p>\\n\\n<p>However, once I let SPSS perform an OLS on these variables, it throws out most of the IV''s.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/qlpWP.png\" alt=\"enter image description here\"></p>\\n\\n<p>The resulting model is almost useless in explaining any of the variance in the DV. My questions are:</p>\\n\\n<ol>\\n<li>Why is SPSS throwing out these variables?</li>\\n<li>How can I prevent this from happening?</li>\\n</ol>\\n\\n<p><img src=\"http://i.stack.imgur.com/iwd0K.png\" alt=\"enter image description here\"></p>\\n\\n<p><code>[edit]</code></p>\\n\\n<p>Added regression model in response to @Penguin_Night''s and @NickCox''s answers:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/UxFK3.png\" alt=\"enter image description here\"></p>\\n','\\n',char(10)", 10776, "2013-05-07 14:59:21.0", 8, "None", 1650, "Why does SPSS throw out variables in OLS (and how can I prevent this from happening)?", "<regression><spss><least-squares>", 1, 1, 10776, "2013-05-07 13:38:09.0"], [83067, 2, "2014-01-22 22:46:33.0", 4, "eplace('<p>I think it has been pretty much covered by whuber, but I just wish to expand on the use of $n-1$; where it comes from and whether it applies here. </p>\\n\\n<p>In an ordinary sample variance, many people use an $n-1$ denominator to make the  usual sum-of-squares-based variance estimate unbiased (not everyone prefers unbiasedness to other properties though). </p>\\n\\n<p>Presumably whoever wrote that formula has concluded that the same should be done with the usual variance estimate for a binomial proportion, which is generally estimated as $p(1-p)/n$ (where $p$ is the sample proportion).</p>\\n\\n<p>Can we see whether the expectation of the usual estimator of variance is the population value?</p>\\n\\n<p>Take $\\\\pi$ to be the corresponding population proportion. That is, does $\\\\text{E}[p(1-p)/n]=\\\\pi(1-\\\\pi)/n$?</p>\\n\\n<p>Equivalently, does $\\\\text{E}[p(1-p)]=\\\\pi(1-\\\\pi)$?</p>\\n\\n<p>Note that if $X$ is the observed count, $p = X/n$, where under the usual sampling assumptions, $X\\\\sim \\\\text{binomial}(n,\\\\pi)$.</p>\\n\\n<p>\\\\begin{eqnarray}\\n\\\\text{E}[p(1-p)] &amp;=&amp; 1/n^2 {E}(X(n-X))\\\\\\\\\\n&amp;=&amp; 1/n^2 (nEX - EX^2) \\\\\\\\\\n&amp;=&amp; 1/n^2 (n^2\\\\pi - n\\\\pi(1-\\\\pi) - n^2\\\\pi^2 )\\\\\\\\\\n&amp;=&amp; 1/n^2 (n^2\\\\pi - n\\\\pi +n\\\\pi^2 - n^2\\\\pi^2 )\\\\\\\\\\n&amp;=&amp; 1/n^2 \\\\cdot n\\\\pi(n - 1 +\\\\pi - n\\\\pi )\\\\\\\\\\n&amp;=&amp; 1/n^2 \\\\cdot n\\\\pi(n - 1)(1-\\\\pi)\\\\\\\\\\n&amp;=&amp; \\\\frac{n-1}{n} \\\\pi (1-\\\\pi)\\n\\\\end{eqnarray}</p>\\n\\n<p>Hence $\\\\text{E}[p(1-p)/(n-1)]=\\\\pi (1-\\\\pi)/n$</p>\\n\\n<p>It looks like (assuming I made no errors) it is the case here too - that the usual estimator of the variance of the proportion is biased, and may be unbiased by multiplying the typical estimator by $\\\\frac{n}{n-1}$.</p>\\n\\n<p>Which means it appears that the formula you have has been chosen to give an unbiased estimate.</p>\\n\\n<p>(I wonder why people seem happy to use a biased variance estimate for binomials when there''s such an insistence on using an unbiased one in other situations. I have no good answer for that; I''ll continue using biased estimators whenever it makes sense to me.)</p>\\n','\\n',char(10)", 805, "2014-01-23 00:08:39.0", 0, 83033, "None", "None", "None", "None", "None", 805, "2014-01-23 00:08:39.0"], [62628, 1, "2013-06-26 11:39:02.0", 0, "eplace('<p>In a standard mean regression setting $y_i=x_i^T \\\\beta + \\\\epsilon_i$, if we assume the errors $\\\\epsilon_i$ are stationary with some covariance  $\\\\Sigma$, what can be said about $x_i$? Is $x_i$ also stationary? Also, is $\\\\epsilon_i|x_i$ stationary? What if $x_i$ includes nonstationary covariates, what can be said about $\\\\epsilon_i$.</p>\\n','\\n',char(10)", 27321, "2013-06-26 11:39:02.0", 3, "None", 75, "Regression with correlated errors", "<regression><correlation>", 0, "None", "None", "None"], [47782, 2, "2013-01-15 16:41:13.0", 113, "eplace('<p>The short version is that the Beta distribution can be understood as representing a distribution <em>of probabilities</em>- that is, it represents all the possible values of a probability when we don''t know what that probability is. Here is my favorite intuitive explanation of this:</p>\\n\\n<p>Anyone who follows baseball is familiar with <a href=\"http://en.wikipedia.org/wiki/Batting_average#Major_League_Baseball\">batting averages</a>- simply the number of times a player gets a base hit divided by the number of times he goes up at bat (so it''s just a percentage between <code>0</code> and <code>1</code>). <code>.266</code> is in general considered an average batting average, while <code>.300</code> is considered an excellent one.</p>\\n\\n<p>Imagine we have a baseball player, and we want to predict what his season-long batting average will be. You might say we can just use his batting average so far- but this will be a very poor measure at the start of a season! If a player goes up to bat once and gets a single, his batting average is briefly <code>1.000</code>, while if he strikes out or walks, his batting average is <code>0.000</code>. It doesn''t get much better if you go up to bat five or six times- you could get a lucky streak and get an average of <code>1.000</code>, or an unlucky streak and get an average of <code>0</code>, neither of which are a remotely good predictor of how you will bat that season.</p>\\n\\n<p>Why is your batting average in the first few hits not a good predictor of your eventual batting average? When a player''s first at-bat is a strikeout, why does no one predict that he''ll never get a hit all season? Because we''re going in with <em>prior expectations.</em> We know that in history, most batting averages over a season have hovered between something like <code>.215</code> and <code>.360</code>, with some extremely rare exceptions on either side. We know that if a player gets a few strikeouts in a row at the start, that might indicate he''ll end up a bit worse than average, but we know he probably won''t deviate from that range.</p>\\n\\n<p>Given our batting average problem, which can be represented with a <a href=\"http://en.wikipedia.org/wiki/Binomial_distribution\">binomial distribution</a> [A series of Successes and Fails], the best way to represent these prior expectations (what we in statistics just call a <a href=\"http://en.wikipedia.org/wiki/Prior_probability\">prior</a>) is with the Beta distribution- it''s saying, before we''ve seen the player take his first swing, what we roughly expect his batting average to be. The domain of the Beta distribution is <code>(0, 1)</code>, just like a probability, so we already know we''re on the right track- but the appropriateness of the Beta for this task goes far beyond that.</p>\\n\\n<p>We expect that the player''s season-long batting average will be most likely around <code>.27</code>, but that it could reasonably range from <code>.21</code> to <code>.35</code>. This can be represented with a Beta distribution with parameters $\\\\alpha=81$ and $\\\\beta=219$:</p>\\n\\n<pre><code>curve(dbeta(x, 81, 219))\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/RJDrz.png\" alt=\"Beta(81, 219)\"></p>\\n\\n<p>I came up with these parameters for two reasons:</p>\\n\\n<ul>\\n<li>The mean is $\\\\frac{\\\\alpha}{\\\\alpha+\\\\beta}=\\\\frac{81}{81+219}=.270$</li>\\n<li>As you can see in the plot, this distribution lies almost entirely within <code>(.2, .35)</code>- the reasonable range for a batting average.</li>\\n</ul>\\n\\n<p>You asked what the x axis represents in a beta distribution density plot- here it represents his batting average. Thus notice that in this case, not only is the y-axis a probability (or more precisely a probability density), but the x-axis is as well (batting average is just a probability of a hit, after all)! The Beta distribution is representing a probability distribution <em>of probabilities</em>.</p>\\n\\n<p>But here''s why the Beta distribution is so appropriate. Imagine the player gets a single hit. His record for the season is now <code>1 hit; 1 at bat</code>. We have to then <em>update</em> our probabilities- we want to shift this entire curve over just a bit to reflect our new information. While the math for proving this is a bit involved (<a href=\"http://en.wikipedia.org/wiki/Conjugate_prior#Example\">it''s shown here</a>), the result is <em>very simple</em>. The new Beta distribution will be:</p>\\n\\n<p>$\\\\mbox{Beta}(\\\\alpha_0+\\\\mbox{hits}, \\\\beta_0+\\\\mbox{misses})$</p>\\n\\n<p>Where $\\\\alpha_0$ and $\\\\beta_0$ are the parameters we started with- that is, 81 and 219. Thus, in this case, $\\\\alpha$  has increased by 1 (his one hit), while $\\\\beta$ has not increased at all (no misses yet). That means our new distribution is $\\\\mbox{Beta}(81+1, 219)$, or:</p>\\n\\n<pre><code>curve(dbeta(x, 82, 219))\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/aULXN.png\" alt=\"enter image description here\"></p>\\n\\n<p>Notice that it has barely changed at all- the change is indeed invisible to the naked eye! (That''s because one hit doesn''t really mean anything).</p>\\n\\n<p>However, the more the player hits over the course of the season, the more the curve will shift to accommodate the new evidence, and furthermore the more it will narrow based on the fact that we have more proof. Let''s say halfway through the season he has been up to bat 300 times, hitting 100 out of those times. The new distribution would be $\\\\mbox{Beta}(81+100, 219+200)$, or:</p>\\n\\n<pre><code>curve(dbeta(x, 82+100, 219+200))\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/oBgYH.png\" alt=\"enter image description here\"></p>\\n\\n<p>Notice the curve is now both thinner and shifted to the right (higher batting average) than it used to be- we have a better sense of what the player''s batting average is.</p>\\n\\n<p>One of the most interesting outputs of this formula is the expected value of the resulting Beta distribution, which is basically your new estimate. Recall that the expected value of the Beta distribution is $\\\\frac{\\\\alpha}{\\\\alpha+\\\\beta}$. Thus, after 100 hits of 300 <em>real</em> at-bats, the expected value of the new Beta distribution is $\\\\frac{82+100}{82+100+219+200}=.303$- notice that it is lower than the naive estimate of $\\\\frac{100}{100+200}=.333$, but higher than the estimate you started the season with ($\\\\frac{81}{81+219}=.270$). You might notice that this formula is equivalent to adding a \"head start\" to the number of hits and non-hits of a player- you''re saying \"start him off in the season with 81 hits and 219 non hits on his record\").</p>\\n\\n<p>Thus, the Beta distribution is best for representing a probabilistic distribution <em>of probabilities</em>- the case where we don''t know what a probability is in advance, but we have some reasonable guesses.</p>\\n','\\n',char(10)", 8373, "2013-12-30 20:25:44.0", 14, 47771, "None", "None", "None", "None", "None", 36727, "2013-12-30 20:25:44.0"], [32182, 2, "2012-07-12 18:01:17.0", 3, "eplace('<p>Maybe there is a better way, but the only way I know is to remove the observations with tumor size X from the dataset, then remove the unused level X from the variable definitions (with <code>droplevels()</code> for example) and then run <code>mice()</code></p>\\n\\n<p><strong>Update1:</strong>\\nOK, there was a typo in my comment last night - I meant to change that line to <code>dfimp2=dfimp[dfimp$overall_tumor_grade!=''X'' | is.na(dfimp$overall_tumor_grade),]</code>  Sorry about that. Anyway, here''s my approach to this problem, based on your simulated dataset:</p>\\n\\n<pre><code>set.seed(100)\\ndf=data.frame(age=rnorm(mean=45,sd=10,25), \\noverall_tumor_grade=factor(sample(c(1,2,3,''X''),25,replace=TRUE)), \\ntumor_size=runif(25)*10) \\ndf[df$overall_tumor_grade==''X'',''tumor_size'']=0 #Patients with Grade ''X'' have tumorsz=0 \\ndf[sample(1:25,3),''age'']=NA   ##Setting some observations to NA \\ndf[sample(1:25,5),''overall_tumor_grade'']=NA \\ndf[sample(1:25,1),''tumor_size'']=NA \\n\\n# output the data\\ndf\\n\\n        age overall_tumor_grade tumor_size\\n1        NA                   2         NA\\n2  46.31531                   1  8.5665304\\n3  44.21083                   1  7.7477889\\n4  53.86785                   2  8.3402710\\n5  46.16971                   3  0.9151028\\n6  48.18630                   2  4.5952549\\n7  39.18209                   1  5.9939816\\n8  52.14533                   1  9.1972191\\n9        NA                   3  9.8282408\\n10       NA                &lt;NA&gt;  0.3780258\\n11 45.89886                   2  5.7793740\\n12 45.96274                   3  7.3331417\\n13 42.98366                   X  0.0000000\\n14 52.39840                &lt;NA&gt;  3.0073652\\n15 46.23380                &lt;NA&gt;  7.3346670\\n16 44.70683                &lt;NA&gt;  9.0695438\\n17 41.11146                   2  2.0981677\\n18 50.10856                   2  3.5813799\\n19 35.86186                   1  4.4829914\\n20 68.10297                   3  9.0642643\\n21 40.61910                   2  3.8943930\\n22 52.64061                &lt;NA&gt;  5.1745975\\n23 47.61961                   3  1.2523909\\n24 52.73405                   X  0.0000000\\n25 36.85621                   3  7.7180549\\n\\n# rows 13 and 24 have tumor grade X\\n\\n# vector for which rows do not have tumor grade X\\nvx &lt;- df$overall_tumor_grade!=''X'' | is.na(df$overall_tumor_grade) \\n# remove those with tumor grade X\\ndfimp &lt;- df[vx,]\\n\\n# do the imputations\\ndfimp$overall_tumor_grade &lt;- droplevels(dfimp$overall_tumor_grade)\\nimp=mice(dfimp, printFlag=F) \\n\\n# add back the rows we removed\\n(new_df1 &lt;- rbind(complete(imp,1),df[!vx,]))\\n\\n1  40.61910                   2  3.8943930\\n2  46.31531                   1  8.5665304\\n3  44.21083                   1  7.7477889\\n4  53.86785                   2  8.3402710\\n5  46.16971                   3  0.9151028\\n6  48.18630                   2  4.5952549\\n7  39.18209                   1  5.9939816\\n8  52.14533                   1  9.1972191\\n9  45.89886                   3  9.8282408\\n10 52.14533                   3  0.3780258\\n11 45.89886                   2  5.7793740\\n12 45.96274                   3  7.3331417\\n14 52.39840                   2  3.0073652\\n15 46.23380                   3  7.3346670\\n16 44.70683                   2  9.0695438\\n17 41.11146                   2  2.0981677\\n18 50.10856                   2  3.5813799\\n19 35.86186                   1  4.4829914\\n20 68.10297                   3  9.0642643\\n21 40.61910                   2  3.8943930\\n22 52.64061                   3  5.1745975\\n23 47.61961                   3  1.2523909\\n25 36.85621                   3  7.7180549\\n13 42.98366                   X  0.0000000\\n24 52.73405                   X  0.0000000\\n</code></pre>\\n','\\n',char(10)", 7486, "2012-07-13 07:37:15.0", 5, 32179, "None", "None", "None", "None", "None", 7486, "2012-07-13 07:37:15.0"], [77653, 2, "2013-11-25 18:15:32.0", 2, "eplace('<p>Probably the best way to gain an understanding is through graphs and through example computations. You don''t say how many other variables you have in the model, nor whether they are continuous or categorical or what, but....</p>\\n\\n<p>Suppose you have a model:</p>\\n\\n<p>$Y = 12 + 3x_1^{\\\\frac{1}{3}} + 2x_2$</p>\\n\\n<p>You could calculate predicted Y at various typical levels of $x_1$ and $x_2$, you could also graph how predicted Y changes: One such graph would have $x_1$ (not transformed) on the x-axis, predicted Y on the y-axis, and a line for the result at each of (say) three levels of $x_2$. If you have many x variables, you might need to just use their median levels in the plot, or make multiple plots. </p>\\n','\\n',char(10)", 686, "2013-11-25 18:15:32.0", 0, 77650, "None", "None", "None", "None", "None", "None", "None"]], "SELECT * FROM votes;": [[43338, 10109, 5, "2011-04-29", 3162], [3084, 914, 2, "2010-07-29", "None"], [21468, 4911, 2, "2010-11-30", "None"], [23242, 5443, 2, "2010-12-13", "None"], [11620, 3185, 2, "2010-09-30", "None"], [26970, 6320, 2, "2011-01-17", "None"]], "SELECT * FROM Tags;": [[1294, "absolute-risk", 5, 63376, 63375], [59, "random-variable", 316, 33480, 33479], [1264, "geometry", 18, "None", "None"]], "SELECT * FROM UserActivityLog;": [[5078, 1347, "Editor", "2011-01-25 04:26:53.0"], [67431, 40359, "Informed", "2014-02-16 09:43:24.0"]], "SELECT * FROM PostVotes;": [[12759, 1844, 2, "2010-10-08"], [470, 5, 2, "2010-07-20"], [39730, 9303, 2, "2011-04-07"], [15210, 3819, 2, "2010-10-21"]], "SELECT * FROM Votes;": [[5799, 300, 2, "2010-08-18"], [33917, 7856, 2, "2011-03-03"], [3619, 759, 2, "2010-08-02"], [25345, 5924, 2, "2011-01-04"], [6688, 2064, 2, "2010-08-25"], [9740, 2698, 2, "2010-09-16"], [12036, 3252, 2, "2010-10-02"]], "SELECT * FROM users;": [[35597, 6, "2013-12-03 00:09:41.0", "SonicFancy", "2013-12-07 04:39:54.0", "Edmonton, Canada", "eplace('<p>Freshman in programming</p>\\n','\\n',char(10)", 5, 0, 0, 3388699, "https://www.gravatar.com/avatar/c74348149d78f46ee9ddaed0b7893c05?s=128&d=identicon&r=PG&f=1", "None", "None"], [31129, 1, "2013-10-06 05:08:19.0", "sasha", "2014-07-27 10:06:13.0", "None", "None", 1, 0, 0, 3396780, "https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1", "None", "None"], [28015, 1, "2013-07-14 16:57:05.0", "DanielD", "2014-04-30 08:21:02.0", "None", "None", 0, 0, 0, 3045502, "None", "None", "None"], [54958, 1, "2014-08-30 15:52:47.0", "frank.liu", "2014-08-31 01:55:16.0", "None", "None", 0, 0, 0, 1560690, "https://www.gravatar.com/avatar/235a8ee66df3538d8576118a26179567?s=128&d=identicon&r=PG", "None", "None"], [43532, 1, "2014-04-10 09:34:26.0", "user_123456", "2014-04-10 09:50:25.0", "None", "None", 0, 0, 0, 4306607, "None", "None", "None"], [6263, 101, "2011-09-11 15:39:31.0", "Mike Glenn", "2011-09-11 15:39:31.0", "United States", "Code Monkey", 7, 2, 0, 22897, "None", "http://www.ilude.com/", 39], [10962, 1, "2012-04-29 22:07:24.0", "Sam35", "2012-05-12 11:46:54.0", "Netherlands", "None", 0, 0, 0, 1418414, "None", "None", "None"], [27463, 1, "2013-06-29 13:44:26.0", "Jeffrey Edwards", "2013-06-29 13:44:26.0", "None", "None", 1, 0, 0, 2985824, "None", "None", "None"], [42002, 71, "2014-03-16 01:47:14.0", "jgstat", "2014-03-20 13:16:51.0", "None", "None", 2, 4, 0, 4179085, "https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1", "None", "None"], [42022, 1, "2014-03-16 15:55:04.0", "chain ro", "2014-08-06 10:07:16.0", "None", "None", 1, 0, 0, 1351503, "https://www.gravatar.com/avatar/183834401387b06b8aff24f88dcd6cbd?s=128&d=identicon&r=PG", "None", "None"]], "SELECT * FROM SoccerPredictions;": [[144619, 74252, 0, "This is a model that i am using to model soccer scores, so i and j are home and away team that are playing respectively. x and y are the goals scored by the home and away team respectively. I have managed to fix all the other parameters except for p which i have to estimate it via MLE.", "2013-10-31 16:12:30.0", 31268]], "SELECT * FROM Links;": [[1905646, "2012-11-08 18:02:57.0", 36111, 4220, 1], [3337858, "2014-09-05 19:34:07.0", 114474, 54943, 1], [2569269, "2013-07-27 11:52:29.0", 65044, 65548, 1], [1728964, "2012-08-06 18:46:03.0", 33785, 33366, 1], [3062079, "2014-05-23 21:19:34.0", 97246, 59124, 1], [1457493, "2012-02-18 21:11:03.0", 23090, 17186, 1], [2258554, "2013-02-18 03:03:17.0", 36281, 1142, 3], [145, "2010-07-23 16:30:41.0", 548, 539, 1]], "SELECT * FROM TagCounts;": [[1074, "wishart", 24, "None", "None"], [1310, "quadratic-form", 25, "None", "None"], [558, "inter-rater", 84, 40523, 40522]], "SELECT * FROM PostHistory;": [[136305, 4, 43765, "94ea58f0-7c0c-4e5d-9fa5-d9354a07da0e", "2012-11-16 18:46:42.0", 16918, "MSE of filtered noisy signal - Derivation", "edited title", ""]], "SELECT * FROM Questions;": [[81300, 6558, "Popular Question", "2014-06-10 19:01:02.0"], [31835, 5405, "Popular Question", "2012-11-21 03:37:54.0"]], "SELECT * FROM DiscussionTable;": [[82648, 41929, 1, "Your model `m2` assumes equal slopes for the `response ~ age` relationship across groups. I would suggest adding a third model which includes an interaction between the grouping factor and the covariate.", "2012-11-05 16:15:13.0", 930]], "SELECT * FROM StudyTags;": [[1424, "crossover-study", 7, 44810, 44809]], "SELECT * FROM TaggedPosts;": [[423, "kalman-filter", 101, 62164, 62163]], "SELECT * FROM Experiments;": [[218668, 2, 66663, "315c4017-ab96-4839-a89a-9c38f6fab7e4", "2013-08-06 16:31:29.0", 28872, "\\\\nWe''re designing a trial to test four factors (call them A, B, C and D) for an experiment. Each factor have \"+\" and \"-\" two levels. The \"+\" of factor A is believed to lead to better results and \"-\" of factor B is also believed to lead to better results. We know nothing about C and D. And don''t know whether there exist any interatcion effect or not. \\\\n\\\\nWe want to get the following goals:\\\\n 1.Find the combination of factor to get the best result.\\\\n 2.Build a model containing A,B,C and D to predict the result of experiments.(This requires A and B should be still in this design)\\\\n\\\\nHow can we get a fractional factorial design basing on the information about A and B?", "", ""]]}