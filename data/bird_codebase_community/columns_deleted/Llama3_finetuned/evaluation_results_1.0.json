{"SELECT * FROM users_status_historys_in_users_table_status_table_status_id_table_status_name_status_code_status_description_table_status_icon_table_status_icon_type_table_status_icon_name_table;": [[40350, 7, "2014-02-16 06:50:34.0", "logamadi", "2014-03-30 17:37:44.0", 4, 0, 0, 3429933, "https://www.gravatar.com/avatar/b558bf7e090c40ec0455fdabb831b31e?s=128&d=identicon&r=PG&f=1"]], "SELECT * FROM tags_values_status_events_in_event_calendar_table_events_table_status_in_event_calendar_table_tags_table_event_calendar_table_status_events_in_event_calendar_table_event_calendar_status_events;": [[1435, "pymc", 54, 46637, 46636]], "SELECT * FROM users2friends2friends2friends2friends2friends2friends2friends2friends2friends2friends2friends2friends2friends2friends2;": [[48749, 1, "2014-06-20 21:26:13.0", "user3761639", "2014-07-01 00:28:38.0", 0, 0, 0, 4642102, "https://www.gravatar.com/avatar/769a4a9450283f19a6aaa33194038e1f?s=128&d=identicon&r=PG&f=1"]], "SELECT * FROM comments_in_events_in_users_in_groups_in_topics_in_categories_in_tags_in_events_in_groups_in_topics_in_categories_in_tags_in_events_in_groups_in_topics_in;": [[82386, 41804, 0, "How is your approach different from Orges'' answer? Also the what if the assumption is not valid; plz see my update.", "2012-11-03 15:21:37.0", 13473]], "SELECT * FROM postLinksValuesCounters2Table1Table1Table2Table3Table4Table5Table6Table7Table8Table9Table10Table11;": [[2638298, "2013-09-13 15:28:32.0", 118, 3734, 1]], "SELECT * FROM postLinks2ndLevelLinksTable2ndLevelLinksTable2ndLevelLinksTable2ndLevelLinksTable2ndLevelLinksTable2ndLevel;": [[906579, "2011-08-05 06:07:07.0", 13873, 12495, 1]], "SELECT * FROM votes_in_elections_in_states_in_countries_in_the_world_in_2011_in_the_year_in_history_in_2011_in_history_in_2011;": [[30228, 315, 2, "2011-02-10"]], "SELECT * FROM postHistory2baseballTable2;2;2014-02-16_06:50:34.0;logamadi;201;": [[129020, 2, 40979, "0f599233-264e-48db-83ce-7654889534b6", "2012-10-22 20:20:20.0", 919, "It appears that \"equitable\" means that the expected values of the team points, conditional on $n$, need to be equal regardless of team size.  This answer explores some consequences of this interpretation.\\\\n\\\\n-----------------------\\\\n\\\\nFor a team of size $n$ that is awarded $x(n)$ points if it \"wins\" and zero points otherwise, the expectation reduces simply to the probability of winning multiplied by $x(n)$.  With the independent \"fair coin\" model of choosing colors, wherein $n$ black values are found with probability $\\\\frac{1}{2}\\\\times \\\\frac{1}{2} \\\\times \\\\cdots \\\\times \\\\frac{1}{2} = 2^{-n}$, this expectation equals $x(n)2^{-n}$.  The condition $x(2)=10$ determines **the unique solution**\\\\n\\\\n$$x(n) = 10 \\\\times 2^{n-2}.$$\\\\n\\\\nNotice that this solution has nothing to do with $n$ being random or not.  However, the distribution of team points at the end of the \"competition\" will be strongly determined by the distribution of $n$. To explore this, **we can simulate a competition.**  (The code uses `R`.)\\\\n\\\\nFirst, we specify the number of teams `n.teams`, the number of trials during a competition `n.trials` (which is fixed and equal for all teams in this simulation), and the parameters for a random distribution of team sizes.  In this example, the distribution is Poisson (offset by $1$ to assure positive team sizes) with parameter `lambda`.  We also start the random number generator at a reproducible point.\\\\n\\\\n    lambda <- 3\\\\n    n.trials <- 100\\\\n    n.teams <- 10000\\\\n    set.seed(17)\\\\n\\\\nCreate the teams, assign hat colors to their members for each trial, count up the total number of wins, and award the points accordingly (using the solution $x(n)$):\\\\n\\\\n    n <- 1 + rpois(n.teams, lambda)\\\\n    n.black <- matrix(rbinom(n.trials * n.teams, size=n, p=1/2), nrow=n.teams) == n\\\\n    wins <- apply(n.black, 1, sum)\\\\n    points <- wins * sapply(n, function(n) 10 * 2^(n-2))\\\\n\\\\nHere is a plot of points *versus* team size in this simulation involving 100 trials for each of 10,000 independent teams (averaging `lambda+1` = $4$ people per team).  To make all the data visible, dots on the plot are randomly offset by up to 2.5% of the width and height of the plot.\\\\n\\\\n    jitter <- 0.025\\\\n    xr <- jitter*(range(n) - mean(n))\\\\n    yr <- jitter*(range(points) - mean(points))\\\\n    plot(n + runif(n.teams, xr[1], xr[2]), points + runif(n.teams, yr[1], yr[2]), \\\\n         xlab=\"Team size\", ylab=\"Total points\")\\\\n    abline(coef(lm(points ~ n)), lwd=2, col=\"Blue\", lty=2)\\\\n\\\\n![Plot][1]\\\\n\\\\nThe blue line is the least squares fit to the data.  Its horizontal trend attests to the \"egalitarian\" nature of the solution $x(n)$: the expected number of points is the same regardless of team size.  (In fact, inspecting the least squares fit with `summary(lm(points ~ n))` reveals it has an insignificant--and relatively small--slope.)\\\\n\\\\nPlease notice (a) the extremely skewed distribution of points and (b) the strong tendency for larger teams either to get no points at all or so many points that a single win assures a high standing at the end of the competition. The uniqueness of $x(n)$ shows that  *this behavior is forced on us by the requirements of the problem*.  It nicely illustrates the tradeoff between risk (exhibited as the large ranges of results for larger teams) and reward (apparent as the numbers of points).\\\\n\\\\nIt is instructive to re-run the simulation with different values of the input. When `lambda` grows, for instance, most teams are relatively large, so only a few--and not necessarily the largest--account for almost all the points.\\\\n\\\\n  [1]: http://i.stack.imgur.com/EtMBW.png", "", ""]], "SELECT * FROM votes_in_2014_0_0_0_0_0_0_0_0_0_0_0_0_;": [[36761, 8364, 2, "2011-03-19"]], "SELECT * FROM postLinks2ndDegreeNodes2ndDegreeNodes2ndDegreeNodes2ndDegreeNodes2ndDegreeNodes2ndDegreeNodes2ndDegreeNodes;": [[2388389, "2013-03-21 14:26:33.0", 52803, 34057, 1]], "SELECT * FROM postLinks2_LikesCount2_UsersCount2_ReactionsCount2_Table2_LikesCount_Table2_LikesCount_Table2_Table2_LikesCount_Table2_Table;": [[3312222, "2014-08-21 17:00:44.0", 112705, 13875, 1]], "SELECT * FROM posts_outcome_table_1_1_1_1_1_1_1_1_1_1_1_1_1;": [[112348, 1, "2014-08-18 17:38:46.0", 0, 28, "eplace('<p>I am running a logistic regression model in R using multiply imputed data created using Amelia II, which I am then analyzing using Zelig. I would like to be able to report some measures of goodness-of-fit (e.g. likelihood ratio, pseudo R-squared, Hosmer-Lemeshow), however none are provided in the default Zelig output and I haven''t been able to figure out a way to extract any from the <code>zelig()</code> object. </p>\\n\\n<p>Do measures of goodness-of-fit need to be calculated differently when using multiply imputed datasets? Are there any R packages that are able to do this? I have looked into several packages that provide measures of goodness-of-fit, such as pscl, however they only work on glm objects, not MI objects created when using Amelia and Zelig.</p>\\n\\n<p>Thanks in advance for your help!</p>\\n','\\n',char(10)", 54277, "2014-08-30 12:36:07.0", "Measures of goodness-of-fit using multiply imputed data in Zelig", "<r><regression><logistic><missing-data><multiple-imputation>", 1, 1, 54277, "2014-08-18 18:24:35.0"]], "SELECT * FROM comments_in_questions_table_2_0_0_0_0_0_0_0_0_0_0_0_0;": [[114172, 59582, 2, "Perhaps a more suitable title could be given to this question?", "2013-05-21 05:11:22.0", 24617]], "SELECT * FROM comments_in_answers_2nd_round_of_competition_in_2011_and_2012_and_2014_and_2015_and_2016;": [[36777, 20393, 0, "Good answer. It is worth noting (though obvious) that working with a class of alternatives that is larger than $k$ for some small $k$ can often be computationally prohibitive, let alone if one has to work with an infinite or uncountable number of alternatives, which may also occur in practice. A big plus of the p-value approach is that it is often (usually?) computationally simple/tractable.", "2011-12-31 06:46:14.0", 7008]], "SELECT * FROM comments_in_posts_table_2_0_0_0_0_0_0_0_0_0_0_0_0;": [[70693, 35297, 1, "eplace('@Bogdanovist You can just use latex markup.  I think what you''re looking for is \\n\\n$$\\n\\\\left[\\\\begin{array}{cc}\\n 1 & t \\\\\\\\\\n 0 & 1 \\\\\\\\\\n\\\\end{array}\\\\right]\\n$$\\n\\nwhich isi `$$\\n\\\\left[\\\\begin{array}{cc}\\n 1 & t \\\\\\\\\\n 0 & 1 \\\\\\\\\\n\\\\end{array}\\\\right]\\n$$`','\\n',char(10)", "2012-08-29 07:07:01.0", 1739]], "SELECT * FROM tags_in_paper_judgment_on_competitions_with_random_team_sizes_and_noisy_outcomes_in_machine_learning_algorithms_and_their_applications_in;": [[1258, "kernel-trick", 45]], "SELECT * FROM postLinks2Table2Table2Table2Table2Table2Table2Table2Table2Table2Table2Table2Table2Table2Table;": [[2243486, "2013-02-11 10:52:01.0", 49731, 15526, 1]], "SELECT * FROM postHistory2history2history2history2history2history2history2history2history2history2history2history2history2history2history;": [[353908, 2, 105562, "64532f6b-b185-4393-8510-3d25fa07af7c", "2014-07-02 17:28:44.0", 45985, "Cost for mis-classification can be used as below\\\\n\\\\nfit <- rpart(target ~ .,\\\\ndata=dataset, method=\"class\", parms=list( split=\"information\", loss=matrix(c(0,1,2,0), byrow=TRUE, nrow=2)), control=rpart.control(usesurrogate=0,maxsurrogate=0))\\\\n\\\\nIn the above example, I have considered that the cost of mis-classifying a -ve observation as 1 unit and cost of mis-classifying a +ve example as +ve as 2 units.", "", ""]], "SELECT * FROM postHistory2Table2Table2Table2Table2Table2Table2Table2Table2Table2Table2Table2Table2Table2Table;": [[39597, 4, 13607, "7528cb3e-338e-4f2f-aea5-2164e69d1cc7", "2011-07-31 02:37:08.0", 183, "Can non-random samples be analyzed using standard statistical tests?", "edited tags; edited title", ""]], "SELECT * FROM postHistoryTable2;;": [[107915, 2, 33815, "ab296b9b-e71d-4140-ace4-9945f4dfccd6", "2012-08-07 02:57:20.0", 3262, "X and Y are not correlated (-.01); however, when I place X in a multiple regression predicting Y, alongside three other (related) variables, X and two other variables are significant predictors of Y. Note that the two other variables are significantly correlated with Y.\\\\n\\\\nHow should I interpret these findings? X predicts unique variance in Y, but since these are not correlated, it is somehow difficult to interpret. \\\\n\\\\nI know of opposite cases (i.e., two variables are correlated but regression is not significant) and those are relatively simpler to understand from a theoretical and statistical perspective.  \\\\n\\\\n\\\\n", "", ""]], "SELECT * FROM badges_values_2_1_0_0_0_0_0_0_0_0_0_0_0_;": [[68270, 85, "Nice Question", "2014-02-25 07:11:38.0"]], "SELECT * FROM postHistoryTable;;": [[21510, 5, 8104, "7072cab9-11d4-432e-a4c7-ae9af757f1db", "2011-03-10 04:37:35.0", 1381, "##Background\\\\n\\\\nOne of the most commonly used weak prior on variance is the inverse-gamma with parameters $\\\\alpha =0.001, \\\\beta=0.001$ [(Gelman 2006)][1].\\\\n\\\\nHowever, this distribution has a 90%CI of approximately $[3\\\\times10^{19},\\\\infty]$.\\\\n\\\\n    library(pscl)\\\\n    sapply(c(0.05, 0.95), function(x) qigamma(x, 0.001, 0.001))\\\\n\\\\n    [1] 3.362941e+19          Inf\\\\n\\\\nFrom this, I interpret that the $IG(0.001, 0.001)$ gives a low probability that variance will be very high, and the very low probability  that variance will be less than 1 $P(\\\\sigma<1|\\\\alpha=0.001, \\\\beta=0.001)=0.006$.\\\\n\\\\n    pigamma(1, 0.001, 0.001)\\\\n    [1] 0.006312353\\\\n\\\\n\\\\n##Question\\\\n\\\\nAm I missing something or is this actually an informative prior?\\\\n\\\\n*update* to clarify, the reason that I was considering this ''informative'' is because it claims very strongly that the variance is enormous and well beyond the scale of almost any variance ever measured. \\\\n\\\\n*follow-up* would a meta-analysis of a large number of variance estimates provide a more reasonable prior?\\\\n   \\\\n---\\\\n\\\\n##Reference\\\\n\\\\nGelman 2006. [Prior distributions for variance parameters in\\\\nhierarchical models][1]. Bayesian Analysis 1(3):515\u2013533\\\\n\\\\n[1]:http://www.stat.columbia.edu/~gelman/research/published/taumain.pdf", "added 320 characters in body", "", "None", "None", "None"], ["None", 2, "None", "dde7ee59-1c8a-4280-92ba-e09b2cac9bf9", "2011-03-15 20:31:46.0", 8330, "None", "None", 22214, "I have a multiple response (categories) question (Q11, Q12, Q13, Q14, Q15).  My problem is that the data-entry clerck repeats the answers in the same case. Example\\\\n\\\\n           Q11 Q12 Q13 Q14 Q15\\\\nCase 1 001 003 015 001 022 (001 was type twice)\\\\n\\\\nCase 2 089 032 089 089 014 (089 was typed three times)\\\\n\\\\nI need to find a way to identified the cases with these errors.\\\\n\\\\nThanks,", "", "user3729"]], "SELECT * FROM tags;tags;": [[1600, "musical-data-analysis", 4]], "SELECT * FROM badges;badges;badge;badge;badge;badge;badge;badge;badge;badge;": [[81849, 48444, "Editor", "2014-06-16 13:45:59.0"]], "SELECT * FROM badges_values_table_2;0;2012-07-13_14:40:15.0;Supporter;2012-;": [[22946, 12053, "Supporter", "2012-07-13 14:40:15.0"]], "SELECT * FROM votes2;2;2014-02-16_06:50:34.0;logamadi;2014-03-30;": [[27402, 6421, 2, "2011-01-21"]], "SELECT * FROM votes;": [[25496, 4450, 2, "2011-01-04"]], "SELECT * FROM badges_in_competition_table_2nd_attempt_at_guessing_table_name_in_database_state:;": [[11416, 5594, "Teacher", "2011-09-23 13:19:21.0"]], "SELECT * FROM posts_2nd_stage_modeling_2nd_stage_modeling_2nd_stage_modeling_2nd_stage_modeling_2nd_stage_model;": [[44620, 2, "2012-11-28 17:15:34.0", 1, "eplace('<p>Could you use some sort of generalize additive model, where the dependent variables are relatedto the predictors as a smooth function, like done in the gam() function in R</p>\\n','\\n',char(10)", 8762, "2012-11-28 17:15:34.0", 0, 44616]], "SELECT * FROM tags_in_paper;paper_id;": [[1159, "statistical-learning", 38]], "SELECT * FROM postLinks_2014_02_16_06_50_34_0_0_3429933_https_www_gravatar_com;": [[2495871, "2013-06-04 17:10:54.0", 60856, 44569, 1]], "SELECT * FROM posts_2nd_order_statistics;;": [[111323, 2, "2014-08-10 01:53:28.0", 3, "eplace('<p>A joint distribution has domain $(-\\\\infty, \\\\infty) \\\\times  (-\\\\infty, \\\\infty)$. If we partition each component of the cartesian product in two by selecting some value $x$ and some value $y$, then we get $4$ subsets,</p>\\n\\n<p>$$(-\\\\infty, x] \\\\times  (-\\\\infty, y],\\\\;\\\\;(-\\\\infty, x] \\\\times  [y,\\\\infty),\\\\\\\\\\n[x, \\\\infty) \\\\times  (-\\\\infty, y],\\\\;\\\\;[x, \\\\infty) \\\\times  [y,\\\\infty)$$</p>\\n\\n<p>made up of intersections of two events,</p>\\n\\n<p>$$A = P(X\\\\le x), \\\\;\\\\; B = P(Y\\\\le y)$$</p>\\n\\n<p>and their corresponding complements.</p>\\n\\n<p>Then (as the OP noted in a commnent),</p>\\n\\n<p>$$\\\\Pr(X\\\\ge x, Y\\\\ge y) = P(A^c\\\\cap B^c) = 1 - P(A\\\\cup B)$$</p>\\n\\n<p>$$=1-\\\\big[P(A) + P(B) - P(A\\\\cap B)\\\\big]$$</p>\\n\\n<p>So it appears that by taking the cross-partial derivative of $\\\\Pr(X\\\\ge x, Y\\\\ge y)$ we should again get the joint density. Let''s verify that:</p>\\n\\n<p>$$\\\\Pr(X\\\\ge x, Y\\\\ge y) = \\\\int_x^{\\\\infty}\\\\int_y^{\\\\infty}f(s,t)dtds$$</p>\\n\\n<p>$$\\\\frac {\\\\partial \\\\Pr(X\\\\ge x, Y\\\\ge y)}{\\\\partial y} = \\\\int_x^{\\\\infty} \\\\left(\\\\frac{\\\\partial}{\\\\partial y}\\\\int_y^{\\\\infty}f(s,t)dt\\\\right)ds $$</p>\\n\\n<p>$$=\\\\int_x^{\\\\infty}-f(s,y) ds$$</p>\\n\\n<p>$$\\\\frac {\\\\partial^2 \\\\Pr(X\\\\ge x, Y\\\\ge y)}{\\\\partial y\\\\partial x} = \\\\frac {\\\\partial }{\\\\partial x} \\\\int_x^{\\\\infty}-f(s,y) ds = -\\\\left(-f(x,y)\\\\right) = f(x,y)$$</p>\\n\\n<p>The above  also means that we can obtain the joint pdf from any of the four joint events indicated by the breakdown of the support -but in the other two cases, we should multiply by $-1$.</p>\\n\\n<p>$$\\\\begin{align} f(x,y) =&amp; \\\\frac {\\\\partial^2 \\\\Pr(X\\\\le x, Y\\\\le y)}{\\\\partial y\\\\partial x}\\\\\\\\\\n=&amp;\\\\frac {\\\\partial^2 \\\\Pr(X\\\\ge x, Y\\\\ge y)}{\\\\partial y\\\\partial x}\\\\\\\\\\n=&amp;-\\\\frac {\\\\partial^2 \\\\Pr(X\\\\le x, Y\\\\ge y)}{\\\\partial y\\\\partial x}\\\\\\\\\\n=&amp;-\\\\frac {\\\\partial^2 \\\\Pr(X\\\\ge x, Y\\\\le y)}{\\\\partial y\\\\partial x}\\n\\\\end{align}$$</p>\\n','\\n',char(10)", 28746, "2014-08-10 03:17:46.0", 2, 28746, "2014-08-10 03:17:46.0", 111317]], "SELECT * FROM users_informed_by_statistical_methods_in_data_analysis_in_r_language_and_environment_including_machine_learning_and_data_visualization_with_r_packages_such_as_z;": [[1886, 24, "2010-11-07 05:14:31.0", "fxnikee", "2010-11-07 05:14:31.0", "http://www.analyticstraining.in", "Bangalore, India", "eplace('<p>I am a Analytics consultant at REDWOOD ASSOCIATES. We also provide training on Analytics to students in India.\\nI am also a part time Forex Trader.</p>\\n','\\n',char(10)", 5, 0, 0, 509577, 26]], "SELECT * FROM votes2;7;2014-02-16_06:50:34.0;logamadi;2014-03-30;": [[36964, 8526, 5, "2011-03-20", 1349]], "SELECT * FROM votes2;0;0;0;0;0;0;0;0;0;0;0;0;0;0;": [[22583, 4457, 2, "2010-12-09"]], "SELECT * FROM posts_2nd_level;1;2014-07-17_13:18:47.0;1;eplace('<p>;": [], "SELECT * FROM users_in_competition_with_missing_data_in_zelig_table_2_table_1_table_1_table_2_table_1_table_1_table;": [[44222, 1, "2014-04-21 23:28:01.0", "user144794", "2014-06-08 01:00:17.0", 2, 0, 0, 4361372, "https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1"]], "SELECT * FROM tags_in_citations_journals_and_conferences_in_data_imputation_and_missing_value_estimation_in_machine_learning_and_statistics_in_mathematics_and_computer_sc;": [[290, "data-imputation", 127, 28270, 28269]], "SELECT * FROM users_in_competition_2014_02_16_06_50_34_logamadi_2014_03_30_17_;": [[10616, 101, "2012-04-15 01:44:53.0", "Jonathan Callen", "2014-03-12 23:35:47.0", "http://example.com/", "United States", "eplace('<p>.</p>\\n','\\n',char(10)", 0, 0, 0, 913608, 27]], "SELECT * FROM users_in_competition_2014_07_29_10_36_25_0_10_10_0_156553_0;": [[588, 1034, "2010-07-29 10:36:25.0", "lokheart", "2014-09-05 16:19:18.0", 185, 38, 0, 156553]], "SELECT * FROM badges_in_use_by_users_in_stack_exchange;;": [[17690, 9912, "Scholar", "2012-03-18 22:36:02.0"]], "SELECT * FROM votes_2nd_stage_inconsistent_with_p_value_approach_in_logistic_regression_modeling_of_binary_outcome_variable_in_r_language_using_zelig_package;": [[3563, 1114, 2, "2010-08-02"]], "SELECT * FROM posts_2nd_degree_connections_in_networks_and_graphs_and_machine_learning;post_id;post_type;likes;shares;comments;created_at;": [[16372, 2, "2011-10-02 23:11:12.0", 6, "eplace('<p>All three methods should produce the same result for the house edge.  To check yours, you should find </p>\\n\\n<pre><code>Number 1s     Prob\\n   0        125/216 \\n   1         75/216\\n   2         15/216\\n   3          1/216\\n</code></pre>\\n\\n<p>which would make your expected winnings (for a stake of 1) </p>\\n\\n<p>$$\\\\frac{-1\\\\times 125 + 1\\\\times 75 + 2 \\\\times 15 + 3 \\\\times 1}{216} = - \\\\frac{17}{216} \\\\approx -0.0787.$$</p>\\n\\n<p>You may need to check your calculations again.</p>\\n','\\n',char(10)", 2958, "2011-10-02 23:11:12.0", 1, 16365]], "SELECT * FROM comments;tags;answers;stars;upvotes;downvotes;share;link;name;location;bio;": [[177986, 90942, 0, "+1. What''s the URL for the 3rd edition of Bayesian Data Analysis?", "2014-03-22 02:11:12.0", 24808]], "SELECT * FROM posts_instantiation_of_statistical_concepts_in_r_language_modeling;2;2012-05-03_12:03:40.;": [[27621, 1, "2012-05-03 11:20:49.0", 1, 474, "eplace('<p>I used R <a href=\"http://cran.r-project.org/web/packages/mvpart/index.html\" rel=\"nofollow\">mvpart</a> package to create a multivariate regression tree. This is part of the output:</p>\\n\\n<pre><code>          CP               nsplit         rel error    xerror       xstd\\n1       0.02717093      0         1.0000000 1.0005358 0.03481409\\n2       0.01302184      2         0.9456581 0.9521266 0.03306820\\n\\nNode number 1: 3479 observations,    complexity param=0.02717093\\nMeans=12.94,0.5749,9.375,0.72,1.611,0.973,2.153,0.6209,3.307,3.702,2.422,0.3837,1.499,     Summed MSE=1305.19 \\nleft son=2 (992 obs) right son=3 (2487 obs)\\nPrimary splits:\\n  Dag           splits as  RRLLRRR, improve=0.02478172, (0 missing)\\n  Hoofdberoep    splits as  RLRLLRRLLLLRLL, improve=0.02313676, (0 missing)\\n  Ploegenstelsel splits as  RRLRRRL, improve=0.02191660, (0 missing)\\n  Werksituatie   splits as  LRLR, improve=0.02179270, (0 missing)\\n  Werkuren       splits as  RLRRRR, improve=0.02130351, (0 missing)\\n</code></pre>\\n\\n<p>How do I have to interpret the different ''improve values'' (0.02478172,0.02313676,...) and how are they related to the complexity parameter (0.02717093)?</p>\\n','\\n',char(10)", 9231, "2012-05-03 12:03:40.0", "How to interpret result from mvpart object in R?", "<r><interpretation><cart>", 0, 1, 930, "2012-05-03 12:03:40.0", "M C"]], "SELECT * FROM users_table_2nd_level_2nd_level_2nd_level_2nd_level_2nd_level_2nd_level_2nd_level;": [[43721, 5, "2014-04-13 20:04:45.0", "user43721", "2014-07-17 18:25:20.0", 1, 0, 0, 4321906, "https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1"]], "SELECT * FROM postHistoryTable;:;": [[111764, 58210, 0, "Your point about the number of penguins and chickens is interesting.  And I get what you''re saying.  However, the way that I read the question, the answer wasn''t intended to be so strongly dependent on the number of chickens or penguins.  What would your answer be if the number of chickens was equal to the number of penguins, and that number was large?", "2013-05-05 23:46:57.0", 25184, "None", "None", "None"], [59455, 0, 6547, 0, "2013-05-19 16:04:03.0", 2, "eplace('<p>A possible very practical approach could be calculate the ratio of the survival function of the distribution $\\\\Pr\\\\left(\\\\tilde X \\\\gt  1- \\\\alpha \\\\right)$ against the normal one, showing it is quite far greater. Another approach can be calculating the ratios of percentiles $w_1=\\\\frac{\\\\tilde{x_{99}}-\\\\tilde{x_{50}}}{\\\\tilde{x_{75}}-\\\\tilde{x_{50}}}$ of the distribution $\\\\tilde x$ under interest and dividing it against the normal one quantile values, $w_2=\\\\frac{\\\\tilde{\\\\Phi_{99}}-\\\\tilde{\\\\Phi_{50}}}{\\\\tilde{\\\\Phi_{75}}-\\\\tilde{\\\\Phi_{50}}}$, $\\\\tau=\\\\frac{w_1}{w_2}$.</p>\\n','\\n',char(10)", "2013-05-19 16:04:03.0", 46889], [66518, "None", "None", "Supporter", "2014-02-07 22:31:27.0", 39707, "None", "None", "None"], [190065, "None", "None", "How is this different from coding $X_2$ as zero for those who only attend the event?", 0, 97599, "None", "2014-05-14 01:18:36.0", 32036]], "SELECT * FROM tagsTable;:;": [[1869, "zero-inflated", 1, "None", "None"], [1431, "None", 6, "determinant", "None"], [55139, "None", 20587, "Teacher", "2013-10-04 14:08:17.0"]], "SELECT * FROM 1,_1,_\"1a\",_post',_1a0dpost,_post,_to_post_post,_\"post_to_post2nd;": [], "SELECT * FROM postLinksTable;;": [[533130, "2011-05-01 09:28:25.0", 10191, 5617, 1]]}